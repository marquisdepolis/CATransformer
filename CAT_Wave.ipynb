{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformer plus a simple equations: learning some wave rules\n",
    "import torch\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, StepLR\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn \n",
    "from torch.nn import functional as F\n",
    "from wavefn import WaveFunction\n",
    "\n",
    "batch_size = 8\n",
    "block_size = 64\n",
    "max_iter = 10000\n",
    "epochs = 5\n",
    "eval_interval = 500\n",
    "learning_rate = 1e-4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "n_embed = 128\n",
    "n_head = 8\n",
    "n_layer = 16\n",
    "dropout = 0.2\n",
    "text = []\n",
    "# tokens set as integers\n",
    "tokens = ['0','1','2','3','4','5','6','7','8','9','s','e']\n",
    "\n",
    "# Example usage\n",
    "wave_length = 64\n",
    "\n",
    "# Let's make some waves! Not strictly needed, but repurposing the wave fn so keeping it for now\n",
    "c = 1.0  # Wave speed\n",
    "dx = 0.1  # Spatial step size\n",
    "dt = 0.1  # Time step size\n",
    "wave_fn = WaveFunction(c, dx, dt)\n",
    "\n",
    "vocab_size=len(tokens)\n",
    "stoi = { ch:i for i, ch in enumerate(tokens)}\n",
    "itos = { i:ch for i, ch in enumerate(tokens)}\n",
    "enc = lambda s: [stoi[str(c)] for c in s]\n",
    "dec = lambda l: ''.join([itos[i] for i in l[1:-1]])  # Skipping the first and last items ('s' and 'e')\n",
    "# Define an appropriate size for your validation batch\n",
    "val_batch_size = batch_size  \n",
    "\n",
    "# Function to generate a random initial wave profile\n",
    "def generate_random_wave_profile(size):\n",
    "    return ((np.random.rand(size)*10).astype(int))\n",
    "\n",
    "# Function to simulate wave sequence\n",
    "def simulate_wave_sequence(batch_size, size):\n",
    "    initial_profiles = [generate_random_wave_profile(size) for _ in range(batch_size)]\n",
    "    final_profiles = []\n",
    "    for profile in initial_profiles:\n",
    "        # Adjusted to capture the transformed profile correctly\n",
    "        transformed_profile = wave_fn.simulate_wave_equation(profile)\n",
    "        transformed_profile_clipped = np.clip(transformed_profile, 0, 9).astype(int)\n",
    "        # Convert numpy array to list of strings\n",
    "        transformed_profile_as_str = 's' + ''.join(map(str, transformed_profile_clipped.tolist())) + 'e'\n",
    "        final_profiles.append(transformed_profile_as_str)\n",
    "    return initial_profiles, final_profiles\n",
    "\n",
    "# load data\n",
    "def get_batch(batch_size, size, block_size):\n",
    "    initial_profiles, final_profiles = simulate_wave_sequence(batch_size, size)\n",
    "    X = torch.tensor([enc(profile)[:block_size] for profile in initial_profiles], dtype=torch.long)\n",
    "    Y = torch.tensor([enc(profile)[:block_size] for profile in final_profiles], dtype=torch.long)\n",
    "    return X.to(device), Y.to(device)\n",
    "\n",
    "class DynamicAdjustment(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super().__init__()\n",
    "        self.adjust = nn.Linear(embed_size, 1)\n",
    "\n",
    "    def forward(self, weights, values):\n",
    "        adjustment = torch.sigmoid(self.adjust(values))\n",
    "        return weights * adjustment.expand_as(weights)\n",
    "\n",
    "# single head attention\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embed,head_size,bias=False)\n",
    "        self.query = nn.Linear(n_embed,head_size,bias=False)\n",
    "        self.value = nn.Linear(n_embed,head_size,bias=False)\n",
    "        self.dynamic_adjustment = DynamicAdjustment(head_size)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size,block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        v = self.value(x)\n",
    "        wei = q @ k.transpose(-2,-1) *C**-0.5 # scaled attention\n",
    "        # wei = wei.masked_fill(self.tril[:T,:T]==0,float('-inf')) # decoder block\n",
    "        wei = F.softmax(wei,dim=-1)\n",
    "        wei = self.dynamic_adjustment(wei, v)  # Apply dynamic adjustment based on value\n",
    "        wei = self.dropout(wei)\n",
    "        v = self.value(x)\n",
    "        out = wei@v\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embed,n_embed)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,x):\n",
    "        out =  torch.cat([h(x) for h in self.heads], dim = -1)\n",
    "        out = self.proj(out) # Projection si the linear transformation of the outcome of prev layer\n",
    "        return out\n",
    "\n",
    "class SinusoidalActivation(nn.Module):\n",
    "    def forward(self, x):\n",
    "        # return torch.sin(x)\n",
    "        return x + torch.sin(x) ** 2\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embed):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embed,4* n_embed), \n",
    "            nn.GELU(),\n",
    "            # SinusoidalActivation(),\n",
    "            nn.Linear(4* n_embed, n_embed),\n",
    "            nn.Dropout(dropout),\n",
    "            )\n",
    "        self\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.net(x)\n",
    "\n",
    "class HierarchicalAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size, levels=3):\n",
    "        super().__init__()\n",
    "        self.levels = nn.ModuleList([MultiHeadAttention(num_heads, head_size) for _ in range(levels)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for attention in self.levels:\n",
    "            x = attention(x)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embed, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embed //n_head\n",
    "        self.sa = HierarchicalAttention(n_head,head_size)\n",
    "        self.ffwd = FeedForward(n_embed)\n",
    "        self.ln1 = nn.LayerNorm(n_embed)\n",
    "        self.ln2 = nn.LayerNorm(n_embed)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        attn_output = self.sa(self.ln1(x))\n",
    "        x = x + attn_output  # add & norm for attention\n",
    "        ffwd_output = self.ffwd(self.ln2(x))\n",
    "        x = x + ffwd_output  # add & norm for feedforward\n",
    "        return x\n",
    "\n",
    "# Add a memory module to the model, to help it remember past interactions\n",
    "class ExternalMemory(nn.Module):\n",
    "    def __init__(self, memory_size, feature_dim):\n",
    "        super().__init__()\n",
    "        self.memory = nn.Parameter(torch.randn(memory_size, feature_dim))\n",
    "        self.query_transform = nn.Linear(feature_dim, feature_dim)  # Transform input query\n",
    "\n",
    "    def read(self, query):\n",
    "        query = self.query_transform(query)  # Transform the query\n",
    "        scores = torch.matmul(query, self.memory.t())\n",
    "        weights = F.softmax(scores, dim=-1)\n",
    "        return torch.matmul(weights, self.memory)  # Return weighted sum of memory entries\n",
    "\n",
    "    def write(self, query, data):\n",
    "        self.memory.data = data  # Simplest form of memory update for illustration\n",
    "\n",
    "class CoPEPositionalEmbedding(nn.Module):\n",
    "    def __init__(self, n_embed, max_len):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(n_embed, n_embed)  # Fully connected layer to adjust input embedding dimensions\n",
    "\n",
    "    def forward(self, tok_emb):\n",
    "        # Assuming tok_emb is of shape [batch_size, seq_len, n_embed]\n",
    "        seq_len = tok_emb.size(1)\n",
    "        position_indices = torch.arange(seq_len, device=tok_emb.device).unsqueeze(0).repeat(tok_emb.size(0), 1)\n",
    "        # Create a position embedding based on sequence indices; initially, we convert indices to embeddings\n",
    "        position_encoded = self.fc(tok_emb + self.fc(position_indices.float().unsqueeze(-1).expand(-1, -1, tok_emb.size(-1))))\n",
    "        return position_encoded\n",
    "\n",
    "class LanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
    "        self.position_embedding_table = CoPEPositionalEmbedding(n_embed, block_size)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embed,n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embed)\n",
    "        self.lm_head = nn.Linear(n_embed,vocab_size)\n",
    "        self.apply(self._init_weights)\n",
    "        self.memory = ExternalMemory(100, n_embed)  # Example memory size and feature dimension\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        tok_emb = self.token_embedding_table(idx)\n",
    "        pos_emb = self.position_embedding_table(tok_emb)\n",
    "        memory_read = self.memory.read(tok_emb.mean(dim=1))\n",
    "        x = tok_emb + pos_emb + memory_read.unsqueeze(1).expand(-1, T, -1)\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        # print(f\"logits are shape {logits.shape} are: {logits} for idx: {idx}\")\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(-1, vocab_size)  # Reshape logits to [batch_size * block_size, vocab_size]\n",
    "            targets = targets.view(-1)  # Flatten targets to [batch_size * block_size]\n",
    "            # targets = targets.view(-1, vocab_size)  # For MSE might need this encording.\n",
    "            loss = F.cross_entropy(logits, targets) \n",
    "            # loss = F.mse_loss(logits, F.one_hot(targets, num_classes=vocab_size).float())\n",
    "            # loss = F.l1_loss(logits, F.one_hot(targets, num_classes=vocab_size).float())\n",
    "            # print(f\"logits are shape {logits.shape} are: {loss} for idx: {idx}\")\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self,idx,max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, loss = self(idx_cond)\n",
    "            logits = logits[:,-1,:]\n",
    "            probs = F.softmax(logits,dim=-1)\n",
    "            idx_next = torch.multinomial(probs,num_samples=1)\n",
    "            idx=torch.cat((idx, idx_next), dim = 1)\n",
    "        return idx\n",
    "    \n",
    "model = LanguageModel()\n",
    "m = model.to(device)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "# optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate, alpha=0.99, eps=1e-08, weight_decay=0.01, momentum=0.5, centered=False)\n",
    "# optimizer = torch.optim.Adadelta(model.parameters(), lr=learning_rate, rho=0.9, eps=1e-06, weight_decay=0.01)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "# scheduler = StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=2, verbose=True)\n",
    "loss = None  # Initialize loss variable outside the loop\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for iter in range(max_iter // epochs):  # Distribute iterations across epochs\n",
    "        model.train()\n",
    "        xb, yb = get_batch(batch_size, wave_length, block_size)\n",
    "        logits, loss = model(xb, yb)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        max_norm = 1\n",
    "        # Clip gradients to avoid exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
    "        optimizer.step()v\n",
    "\n",
    "        if iter % eval_interval == 0 and loss is not None:  # Validation logic\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                xv, yv = get_batch(val_batch_size, wave_length, block_size)\n",
    "                val_logits, val_loss = model(xv, yv)\n",
    "                print(f\"Epoch {epoch}, Iteration {iter}: Training Loss = {loss.item()}, Validation Loss = {val_loss.item()}\")\n",
    "            model.train()\n",
    "\n",
    "    scheduler.step(val_loss)  # Update the learning rate at the end of each epoch\n",
    "\n",
    "torch.save(model, 'models/cat_wave_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gen\tnevals\tavg     \tmin     \tmax     \n",
      "0  \t50    \t0.430505\t0.400879\t0.451669\n",
      "1  \t36    \t0.435844\t0.404948\t0.452285\n",
      "2  \t39    \t0.440353\t0.412776\t0.463669\n",
      "3  \t41    \t0.44244 \t0.407125\t0.457798\n",
      "4  \t44    \t0.445278\t0.412934\t0.462742\n",
      "5  \t41    \t0.448383\t0.417657\t0.467237\n",
      "6  \t40    \t0.45012 \t0.4219  \t0.471545\n",
      "7  \t38    \t0.452346\t0.410691\t0.472848\n",
      "8  \t38    \t0.453653\t0.430745\t0.472848\n",
      "9  \t35    \t0.455481\t0.436012\t0.472848\n",
      "10 \t36    \t0.455435\t0.433951\t0.472848\n",
      "Evolution complete\n",
      "Best individual: [{'n_embed': 128, 'n_head': 4, 'n_layer': 6}, {'n_embed': 128, 'n_head': 4, 'n_layer': 6}, {'n_embed': 128, 'n_head': 4, 'n_layer': 4}, {'n_embed': 128, 'n_head': 8, 'n_layer': 4}, {'n_embed': 128, 'n_head': 4, 'n_layer': 4}]\n",
      "Training best ensemble...\n",
      "Epoch 1, Average Loss: 1.781982660293579\n",
      "Epoch 2, Average Loss: 1.7379969662427903\n",
      "Epoch 3, Average Loss: 1.7310946106910705\n",
      "Epoch 4, Average Loss: 1.739592705965042\n",
      "Epoch 5, Average Loss: 1.7339089155197143\n",
      "Training complete\n",
      "Model saved as 'best_evolutionary_ensemble.pth'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from deap import base, creator, tools, algorithms\n",
    "import random\n",
    "\n",
    "# Assuming these are defined in your original code\n",
    "from wavefn import WaveFunction\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 8\n",
    "block_size = 16\n",
    "max_iter = 1000\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "vocab_size = 12  # Assuming this is correct based on your original tokens\n",
    "\n",
    "tokens = ['0','1','2','3','4','5','6','7','8','9','s','e']\n",
    "stoi = { ch:i for i, ch in enumerate(tokens)}\n",
    "itos = { i:ch for i, ch in enumerate(tokens)}\n",
    "enc = lambda s: [stoi[str(c)] for c in s]\n",
    "dec = lambda l: ''.join([itos[i] for i in l[1:-1]])  # Skipping the first and last items ('s' and 'e')\n",
    "\n",
    "# Wave generation functions (from your original code)\n",
    "def generate_random_wave_profile(size):\n",
    "    return ((np.random.rand(size)*10).astype(int))\n",
    "\n",
    "def simulate_wave_sequence(batch_size, size):\n",
    "    initial_profiles = [generate_random_wave_profile(size) for _ in range(batch_size)]\n",
    "    final_profiles = []\n",
    "    for profile in initial_profiles:\n",
    "        transformed_profile = wave_fn.simulate_wave_equation(profile)\n",
    "        transformed_profile_clipped = np.clip(transformed_profile, 0, 9).astype(int)\n",
    "        transformed_profile_as_str = 's' + ''.join(map(str, transformed_profile_clipped.tolist())) + 'e'\n",
    "        final_profiles.append(transformed_profile_as_str)\n",
    "    return initial_profiles, final_profiles\n",
    "\n",
    "def get_batch(batch_size, size, block_size):\n",
    "    initial_profiles, final_profiles = simulate_wave_sequence(batch_size, size)\n",
    "    X = torch.tensor([enc(profile)[:block_size] for profile in initial_profiles], dtype=torch.long)\n",
    "    Y = torch.tensor([enc(profile)[:block_size] for profile in final_profiles], dtype=torch.long)\n",
    "    return X.to(device), Y.to(device)\n",
    "\n",
    "# Simplified model components\n",
    "class CoPEPositionalEmbedding(nn.Module):\n",
    "    def __init__(self, n_embed, max_len):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(n_embed, n_embed)\n",
    "\n",
    "    def forward(self, tok_emb):\n",
    "        seq_len = tok_emb.size(1)\n",
    "        position_indices = torch.arange(seq_len, device=tok_emb.device).unsqueeze(0).repeat(tok_emb.size(0), 1)\n",
    "        position_encoded = self.fc(tok_emb + self.fc(position_indices.float().unsqueeze(-1).expand(-1, -1, tok_emb.size(-1))))\n",
    "        return position_encoded\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embed, n_head):\n",
    "        super().__init__()\n",
    "        self.sa = nn.MultiheadAttention(n_embed, n_head)\n",
    "        self.ffwd = nn.Sequential(\n",
    "            nn.Linear(n_embed, 4 * n_embed),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * n_embed, n_embed)\n",
    "        )\n",
    "        self.ln1 = nn.LayerNorm(n_embed)\n",
    "        self.ln2 = nn.LayerNorm(n_embed)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x), self.ln1(x), self.ln1(x))[0]\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class SmallLLM(nn.Module):\n",
    "    def __init__(self, vocab_size, n_embed, n_head, n_layer):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, n_embed)\n",
    "        self.position_embedding = CoPEPositionalEmbedding(n_embed, block_size)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embed, n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embed)\n",
    "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
    "\n",
    "    def forward(self, idx):\n",
    "        tok_emb = self.token_embedding(idx)\n",
    "        pos_emb = self.position_embedding(tok_emb)\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "        return logits\n",
    "\n",
    "class LLMEnsemble(nn.Module):\n",
    "    def __init__(self, models):\n",
    "        super().__init__()\n",
    "        self.models = nn.ModuleList(models)\n",
    "        self.weights = nn.Parameter(torch.ones(len(models)) / len(models))\n",
    "\n",
    "    def forward(self, idx):\n",
    "        outputs = [model(idx) for model in self.models]\n",
    "        weighted_sum = sum(w * out for w, out in zip(self.weights, outputs))\n",
    "        return weighted_sum\n",
    "\n",
    "# Evolutionary Algorithm setup\n",
    "creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
    "creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
    "\n",
    "def generate_random_architecture():\n",
    "    return {\n",
    "        'n_embed': random.choice([32, 64, 128]),\n",
    "        'n_head': random.choice([2, 4, 8]),\n",
    "        'n_layer': random.choice([2, 4, 6])\n",
    "    }\n",
    "\n",
    "def mutate(individual):\n",
    "    idx = random.randint(0, len(individual) - 1)\n",
    "    individual[idx] = generate_random_architecture()\n",
    "    return individual,\n",
    "\n",
    "def mate(ind1, ind2):\n",
    "    return tools.cxTwoPoint(ind1, ind2)\n",
    "\n",
    "def evaluate(individual):\n",
    "    models = [SmallLLM(vocab_size, **arch) for arch in individual]\n",
    "    ensemble = LLMEnsemble(models).to(device)\n",
    "    optimizer = torch.optim.AdamW(ensemble.parameters(), lr=1e-4)\n",
    "    \n",
    "    total_loss = 0\n",
    "    for _ in range(10):  # Short training\n",
    "        xb, yb = get_batch(batch_size, 64, block_size)\n",
    "        logits = ensemble(xb)\n",
    "        loss = F.cross_entropy(logits.view(-1, vocab_size), yb.view(-1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return (1 / (total_loss / 10),)  # Return fitness (higher is better)\n",
    "\n",
    "toolbox = base.Toolbox()\n",
    "toolbox.register(\"individual\", tools.initRepeat, creator.Individual, generate_random_architecture, n=5)\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "toolbox.register(\"mate\", mate)\n",
    "toolbox.register(\"mutate\", mutate)\n",
    "toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
    "toolbox.register(\"evaluate\", evaluate)\n",
    "\n",
    "def train_best_ensemble(best_individual, epochs=5):\n",
    "    models = [SmallLLM(vocab_size, **arch) for arch in best_individual]\n",
    "    ensemble = LLMEnsemble(models).to(device)\n",
    "    optimizer = torch.optim.AdamW(ensemble.parameters(), lr=1e-4)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=2, verbose=True)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for iter in range(max_iter // epochs):\n",
    "            xb, yb = get_batch(batch_size, 64, block_size)\n",
    "            logits = ensemble(xb)\n",
    "            loss = F.cross_entropy(logits.view(-1, vocab_size), yb.view(-1))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / (max_iter // epochs)\n",
    "        print(f\"Epoch {epoch+1}, Average Loss: {avg_loss}\")\n",
    "        scheduler.step(avg_loss)\n",
    "\n",
    "    return ensemble\n",
    "\n",
    "def main():\n",
    "    pop = toolbox.population(n=50)\n",
    "    hof = tools.HallOfFame(1)\n",
    "    stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "    stats.register(\"avg\", np.mean)\n",
    "    stats.register(\"min\", np.min)\n",
    "    stats.register(\"max\", np.max)\n",
    "\n",
    "    pop, logbook = algorithms.eaSimple(pop, toolbox, cxpb=0.7, mutpb=0.2, ngen=10, stats=stats, halloffame=hof, verbose=True)\n",
    "\n",
    "    print(\"Evolution complete\")\n",
    "    best_individual = hof[0]\n",
    "    print(\"Best individual:\", best_individual)\n",
    "\n",
    "    print(\"Training best ensemble...\")\n",
    "    best_ensemble = train_best_ensemble(best_individual)\n",
    "    \n",
    "    print(\"Training complete\")\n",
    "    return best_ensemble\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize WaveFunction (assuming this is from your original code)\n",
    "    wave_length = 32\n",
    "    c = 1.0\n",
    "    dx = 0.1\n",
    "    dt = 0.1\n",
    "    wave_fn = WaveFunction(c, dx, dt)\n",
    "\n",
    "    # Run the evolutionary process and train the best ensemble\n",
    "    best_model = main()\n",
    "\n",
    "    # Save the model\n",
    "    torch.save(best_model.state_dict(), 'models/best_evolutionary_ensemble.pth')\n",
    "    print(\"Model saved as 'best_evolutionary_ensemble.pth'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'collections.OrderedDict' object has no attribute 'eval'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Load the model\u001b[39;00m\n\u001b[1;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodels/best_evolutionary_ensemble.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval\u001b[49m()\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# np.random.seed(42)  # Seed for reproducibility\u001b[39;00m\n\u001b[1;32m      7\u001b[0m input_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m16\u001b[39m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'collections.OrderedDict' object has no attribute 'eval'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "# Load the model\n",
    "model = torch.load('models/best_evolutionary_ensemble.pth')\n",
    "model.eval()\n",
    "# np.random.seed(42)  # Seed for reproducibility\n",
    "input_length = 16\n",
    "input = (np.random.rand(input_length)*10).astype(int)\n",
    "print(f\"Input is: {input}\")\n",
    "encoded_input = enc(input)\n",
    "\n",
    "# Convert to a tensor and add a batch dimension (assuming your model expects a batch)\n",
    "input_tensor = torch.tensor([encoded_input], dtype=torch.long).to(device)\n",
    "max_new_tokens = input_length  # Define how many new tokens you want to generate\n",
    "output_tensor = model.generate(input_tensor, max_new_tokens=max_new_tokens)\n",
    "decoded_output = dec(output_tensor[0].tolist())  # Assuming you're interested in the first item in the batch\n",
    "generated_output = decoded_output[input_length+1:].split('e')[0]\n",
    "sim_wave_output = wave_fn.simulate_wave_equation(input)\n",
    "print(\"Decoded Output:\", decoded_output)\n",
    "print(\"Generated Output:\", generated_output)\n",
    "print(\"Wavefn Output:\", sim_wave_output)\n",
    "\n",
    "# # Plot!\n",
    "decoded_values = [int(char) for char in generated_output]\n",
    "\n",
    "print(\"Decoded values: \", decoded_values)\n",
    "global_min = 0\n",
    "global_max = 9\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Plot for the Transformer Model Output\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(decoded_values, marker='o', linestyle='-')\n",
    "plt.title('Transformer Model Output')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.ylim(global_min, global_max+1)  # Set Y-axis limits to be the same for both plots\n",
    "\n",
    "# Plot for the Wave Function Simulation Output\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(sim_wave_output, marker='o', linestyle='-')\n",
    "plt.title('Wave Function Simulation Output')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Tokens')\n",
    "plt.ylim(global_min, global_max+1)  # Set Y-axis limits to be the same for both plots\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
