{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformer plus evolutionary algorithms: learning some wave rules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from deap import base, creator, tools, algorithms\n",
    "import random\n",
    "\n",
    "from wavefn import WaveFunction\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 8\n",
    "block_size = 16\n",
    "max_iter = 5000\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "vocab_size = 12  # Assuming this is correct based on your original tokens\n",
    "\n",
    "tokens = ['0','1','2','3','4','5','6','7','8','9','s','e']\n",
    "stoi = { ch:i for i, ch in enumerate(tokens)}\n",
    "itos = { i:ch for i, ch in enumerate(tokens)}\n",
    "enc = lambda s: [stoi[str(c)] for c in s]\n",
    "dec = lambda l: ''.join([itos[i] for i in l[1:-1]])  # Skipping the first and last items ('s' and 'e')\n",
    "\n",
    "# Wave generation functions (from your original code)\n",
    "def generate_random_wave_profile(size):\n",
    "    return ((np.random.rand(size)*10).astype(int))\n",
    "\n",
    "def simulate_wave_sequence(batch_size, size):\n",
    "    initial_profiles = [generate_random_wave_profile(size) for _ in range(batch_size)]\n",
    "    final_profiles = []\n",
    "    for profile in initial_profiles:\n",
    "        transformed_profile = wave_fn.simulate_wave_equation(profile)\n",
    "        # Remove clipping to allow for more variation\n",
    "        transformed_profile_as_str = 's' + ''.join(map(lambda x: str(int(x) % 10), transformed_profile.tolist())) + 'e'\n",
    "        final_profiles.append(transformed_profile_as_str)\n",
    "    return initial_profiles, final_profiles\n",
    "\n",
    "def get_batch(batch_size, size, block_size):\n",
    "    initial_profiles, final_profiles = simulate_wave_sequence(batch_size, size)\n",
    "    X = torch.tensor([enc('s' + ''.join(map(str, profile)) + 'e')[:block_size] for profile in initial_profiles], dtype=torch.long)\n",
    "    Y = torch.tensor([enc(profile)[:block_size] for profile in final_profiles], dtype=torch.long)\n",
    "    return X.to(device), Y.to(device)\n",
    "\n",
    "def decode_model_output(logits):\n",
    "    # Convert logits to probabilities and get the most likely token\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    predicted_tokens = torch.argmax(probs, dim=-1)\n",
    "    \n",
    "    # Decode the predicted tokens\n",
    "    decoded_outputs = []\n",
    "    for sequence in predicted_tokens:\n",
    "        decoded_sequence = dec(sequence.tolist())\n",
    "        decoded_outputs.append(decoded_sequence)\n",
    "    \n",
    "    return decoded_outputs\n",
    "\n",
    "class CoPEPositionalEmbedding(nn.Module):\n",
    "    def __init__(self, n_embed, max_len):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(n_embed, n_embed)\n",
    "\n",
    "    def forward(self, tok_emb):\n",
    "        seq_len = tok_emb.size(1)\n",
    "        position_indices = torch.arange(seq_len, device=tok_emb.device).unsqueeze(0).repeat(tok_emb.size(0), 1)\n",
    "        position_encoded = self.fc(tok_emb + self.fc(position_indices.float().unsqueeze(-1).expand(-1, -1, tok_emb.size(-1))))\n",
    "        return position_encoded\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embed, n_head):\n",
    "        super().__init__()\n",
    "        self.sa = nn.MultiheadAttention(n_embed, n_head)\n",
    "        self.ffwd = nn.Sequential(\n",
    "            nn.Linear(n_embed, 4 * n_embed),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * n_embed, n_embed)\n",
    "        )\n",
    "        self.ln1 = nn.LayerNorm(n_embed)\n",
    "        self.ln2 = nn.LayerNorm(n_embed)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x), self.ln1(x), self.ln1(x))[0]\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class SmallLLM(nn.Module):\n",
    "    def __init__(self, vocab_size, n_embed, n_head, n_layer):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, n_embed)\n",
    "        self.position_embedding = CoPEPositionalEmbedding(n_embed, block_size)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embed, n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embed)\n",
    "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
    "\n",
    "    def forward(self, idx):\n",
    "        tok_emb = self.token_embedding(idx)\n",
    "        pos_emb = self.position_embedding(tok_emb)\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "        return logits\n",
    "\n",
    "class LLMEnsemble(nn.Module):\n",
    "    def __init__(self, models):\n",
    "        super().__init__()\n",
    "        self.models = nn.ModuleList(models)\n",
    "        self.weights = nn.Parameter(torch.ones(len(models)) / len(models))\n",
    "\n",
    "    def forward(self, idx):\n",
    "        outputs = [model(idx) for model in self.models]\n",
    "        weighted_sum = sum(w * out for w, out in zip(self.weights, outputs))\n",
    "        return weighted_sum\n",
    "\n",
    "# Evolutionary Algorithm setup\n",
    "creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
    "creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
    "\n",
    "def generate_random_architecture():\n",
    "    return {\n",
    "        'n_embed': random.choice([32, 48, 64, 80, 96, 112, 128, 144, 160]),\n",
    "        'n_head': random.choice([1, 2, 4, 8]),\n",
    "        'n_layer': random.choice([1, 2, 4, 8])\n",
    "    }\n",
    "\n",
    "def mutate(individual):\n",
    "    idx = random.randint(0, len(individual) - 1)\n",
    "    individual[idx] = generate_random_architecture()\n",
    "    return individual,\n",
    "\n",
    "def mate(ind1, ind2):\n",
    "    return tools.cxTwoPoint(ind1, ind2)\n",
    "\n",
    "def evaluate(individual):\n",
    "    models = [SmallLLM(vocab_size, **arch) for arch in individual]\n",
    "    ensemble = LLMEnsemble(models).to(device)\n",
    "    optimizer = torch.optim.AdamW(ensemble.parameters(), lr=1e-4)\n",
    "    \n",
    "    total_loss = 0\n",
    "    for _ in range(10):  # Short training\n",
    "        xb, yb = get_batch(batch_size, 64, block_size)\n",
    "        logits = ensemble(xb)\n",
    "        loss = F.cross_entropy(logits.view(-1, vocab_size), yb.view(-1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return (1 / (total_loss / 10),)  # Return fitness (higher is better)\n",
    "\n",
    "toolbox = base.Toolbox()\n",
    "toolbox.register(\"individual\", tools.initRepeat, creator.Individual, generate_random_architecture, n=5)\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "toolbox.register(\"mate\", mate)\n",
    "toolbox.register(\"mutate\", mutate)\n",
    "toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
    "toolbox.register(\"evaluate\", evaluate)\n",
    "\n",
    "def custom_loss(predictions, targets, vocab_size):\n",
    "    ce_loss = F.cross_entropy(predictions.view(-1, vocab_size), targets.view(-1))\n",
    "    zero_penalty = torch.mean(torch.abs(predictions[:, :, 0]))  # Penalize predicting zero\n",
    "    return ce_loss + 0.1 * zero_penalty\n",
    "\n",
    "def diversity_loss(ensemble_outputs):\n",
    "    mean_output = torch.mean(ensemble_outputs, dim=0)\n",
    "    return -torch.mean(torch.abs(ensemble_outputs - mean_output))\n",
    "\n",
    "def train_best_ensemble(best_individual, epochs=5):\n",
    "    models = [SmallLLM(vocab_size, **arch) for arch in best_individual]\n",
    "    ensemble = LLMEnsemble(models).to(device)\n",
    "    optimizer = torch.optim.AdamW(ensemble.parameters(), lr=1e-4)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=2, verbose=True)\n",
    "\n",
    "    val_batch_size = batch_size\n",
    "    max_norm = 1\n",
    "    eval_interval = 500\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for iter in range(max_iter // epochs):\n",
    "            ensemble.train()\n",
    "            xb, yb = get_batch(batch_size, 64, block_size)\n",
    "            \n",
    "            # Get individual model outputs\n",
    "            individual_outputs = [model(xb) for model in ensemble.models]\n",
    "            \n",
    "            # Compute ensemble output\n",
    "            ensemble_output = ensemble(xb)\n",
    "            \n",
    "            # Compute custom loss\n",
    "            custom_loss_value = custom_loss(ensemble_output, yb, vocab_size)\n",
    "            \n",
    "            # Compute diversity loss\n",
    "            div_loss_value = diversity_loss(torch.stack(individual_outputs))\n",
    "            \n",
    "            # Combine losses\n",
    "            loss = custom_loss_value + 0.1 * div_loss_value  # You can adjust this weight\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(ensemble.parameters(), max_norm)\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            if iter % eval_interval == 0:\n",
    "                ensemble.eval()\n",
    "                with torch.no_grad():\n",
    "                    xv, yv = get_batch(val_batch_size, 64, block_size)\n",
    "                    val_logits = ensemble(xv)\n",
    "                    val_loss = custom_loss(val_logits, yv, vocab_size)\n",
    "                    print(f\"Epoch {epoch}, Iteration {iter}: Training Loss = {loss.item()}, Validation Loss = {val_loss.item()}\")\n",
    "                    \n",
    "                    # Decode and print examples\n",
    "                    input_decoded = [dec(seq.tolist()) for seq in xv[:3]]\n",
    "                    target_decoded = [dec(seq.tolist()) for seq in yv[:3]]\n",
    "                    predicted_decoded = decode_model_output(val_logits[:3])\n",
    "                    \n",
    "                    print(\"Input examples:\", input_decoded)\n",
    "                    print(\"Target examples:\", target_decoded)\n",
    "                    print(\"Predicted examples:\", predicted_decoded)\n",
    "                \n",
    "                ensemble.train()\n",
    "\n",
    "        avg_loss = total_loss / (max_iter // epochs)\n",
    "        print(f\"Epoch {epoch+1}, Average Loss: {avg_loss}\")\n",
    "        scheduler.step(avg_loss)\n",
    "\n",
    "    return ensemble\n",
    "\n",
    "def main():\n",
    "    global best_individual\n",
    "    pop = toolbox.population(n=50)\n",
    "    hof = tools.HallOfFame(1)\n",
    "    stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "    stats.register(\"avg\", np.mean)\n",
    "    stats.register(\"min\", np.min)\n",
    "    stats.register(\"max\", np.max)\n",
    "\n",
    "    pop, logbook = algorithms.eaSimple(pop, toolbox, cxpb=0.7, mutpb=0.2, ngen=10, stats=stats, halloffame=hof, verbose=True)\n",
    "\n",
    "    print(\"Evolution complete\")\n",
    "    best_individual = hof[0]\n",
    "    print(\"Best individual:\", best_individual)\n",
    "\n",
    "    print(\"Training best ensemble...\")\n",
    "    best_ensemble = train_best_ensemble(best_individual)\n",
    "    \n",
    "    print(\"Training complete\")\n",
    "    return best_ensemble\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize WaveFunction (assuming this is from your original code)\n",
    "    wave_length = 32\n",
    "    c = 1.0\n",
    "    dx = 0.1\n",
    "    dt = 0.1\n",
    "    wave_fn = WaveFunction(c, dx, dt)\n",
    "\n",
    "    # Run the evolutionary process and train the best ensemble\n",
    "    best_model = main()\n",
    "\n",
    "    # Save the model\n",
    "    torch.save(best_model.state_dict(), 'models/best_evolutionary_ensemble.pth')\n",
    "    print(\"Model saved as 'best_evolutionary_ensemble.pth'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# Sampling functions\n",
    "def temperature_sampling(logits, temperature=0.7):  # Lowered from 1.0\n",
    "    probs = F.softmax(logits / temperature, dim=-1)\n",
    "    return torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "def calculate_accuracy(predicted, ground_truth):\n",
    "    correct = sum(p == g for p, g in zip(predicted, ground_truth))\n",
    "    return correct / len(ground_truth)\n",
    "\n",
    "def top_k_sampling(logits, k=50):\n",
    "    top_k = torch.topk(logits, k)\n",
    "    indices_to_remove = logits < top_k.values[..., -1, None]\n",
    "    logits[indices_to_remove] = float('-inf')\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    return torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "def nucleus_sampling(logits, p=0.9):\n",
    "    sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "    cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "    sorted_indices_to_remove = cumulative_probs > p\n",
    "    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "    sorted_indices_to_remove[..., 0] = 0\n",
    "    indices_to_remove = sorted_indices_to_remove.scatter(dim=1, index=sorted_indices, src=sorted_indices_to_remove)\n",
    "    logits[indices_to_remove] = float('-inf')\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    return torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "def generate_output(model, input_tensor, max_length=18, sampling_method='temperature', **kwargs):\n",
    "    generated = input_tensor\n",
    "    for _ in range(max_length - input_tensor.size(1)):\n",
    "        with torch.no_grad():\n",
    "            logits = model(generated)[:, -1, :]\n",
    "            if sampling_method == 'temperature':\n",
    "                next_token = temperature_sampling(logits, temperature=kwargs.get('temperature', 0.8))\n",
    "            elif sampling_method == 'top_k':\n",
    "                next_token = top_k_sampling(logits, k=kwargs.get('k', 50))\n",
    "            elif sampling_method == 'nucleus':\n",
    "                next_token = nucleus_sampling(logits, p=kwargs.get('p', 0.9))\n",
    "            else:\n",
    "                next_token = torch.argmax(logits, dim=-1).unsqueeze(0)\n",
    "        generated = torch.cat((generated, next_token), dim=1)\n",
    "    return generated\n",
    "\n",
    "# Load the model (assuming this part is correct)\n",
    "models = [SmallLLM(vocab_size, **arch) for arch in best_individual]\n",
    "model = LLMEnsemble(models).to(device)\n",
    "model.load_state_dict(torch.load('models/best_evolutionary_ensemble.pth'))\n",
    "model.eval()\n",
    "\n",
    "# Generate input\n",
    "input_length = 16\n",
    "input_wave = generate_random_wave_profile(input_length)\n",
    "print(f\"Input wave: {input_wave}\")\n",
    "\n",
    "# Encode input with start and end tokens\n",
    "encoded_input = enc('s' + ''.join(map(str, input_wave)) + 'e')\n",
    "input_tensor = torch.tensor([encoded_input], dtype=torch.long).to(device)\n",
    "\n",
    "# Generate output using different sampling methods\n",
    "sampling_methods = ['argmax', 'temperature', 'top_k', 'nucleus']\n",
    "outputs = {}\n",
    "\n",
    "for method in sampling_methods:\n",
    "    with torch.no_grad():\n",
    "        if method == 'argmax':\n",
    "            logits = model(input_tensor)\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            predicted_tokens = torch.argmax(probs, dim=-1)\n",
    "        else:\n",
    "            predicted_tokens = generate_output(model, input_tensor, sampling_method=method)\n",
    "\n",
    "        print(f\"\\nSampling method: {method}\")\n",
    "        print(\"Logits shape:\", logits.shape)\n",
    "        print(\"Logits mean:\", logits.mean().item())\n",
    "        print(\"Logits std:\", logits.std().item())\n",
    "\n",
    "        # Print probabilities for the first few positions\n",
    "        for i in range(5):\n",
    "            print(f\"Top 5 probable tokens at position {i}:\")\n",
    "            top_probs, top_indices = torch.topk(probs[0, i], 5)\n",
    "            for prob, idx in zip(top_probs, top_indices):\n",
    "                print(f\"  Token: {itos[idx.item()]}, Probability: {prob.item():.4f}\")\n",
    "\n",
    "        # Decode the entire output, including start and end tokens\n",
    "        decoded_output = ''.join([itos[i] for i in predicted_tokens[0].tolist()])\n",
    "        print(\"Raw decoded output:\", decoded_output)\n",
    "\n",
    "        # Extract the actual wave values (remove 's' and 'e', and convert to integers)\n",
    "        generated_output = [int(char) for char in decoded_output[1:-1] if char.isdigit()]\n",
    "        print(\"Generated output:\", generated_output)\n",
    "        \n",
    "        # Calculate and print accuracy\n",
    "        accuracy = calculate_accuracy(generated_output, sim_wave_output)\n",
    "        print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "        outputs[method] = generated_output\n",
    "        \n",
    "def analyze_token_distribution(logits):\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    token_dist = probs.mean(dim=(0, 1)).cpu().numpy()\n",
    "    print(\"Token distribution:\")\n",
    "    for token, prob in sorted(zip(itos.values(), token_dist), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"  {token}: {prob:.4f}\")\n",
    "\n",
    "# After loading the model\n",
    "ensemble_weights = model.weights.detach().cpu().numpy()\n",
    "print(\"Ensemble weights:\", ensemble_weights)\n",
    "print(\"Weight distribution:\", np.histogram(ensemble_weights))\n",
    "analyze_token_distribution(logits)\n",
    "\n",
    "# Simulate wave using the original input\n",
    "sim_wave_output = wave_fn.simulate_wave_equation(input_wave)\n",
    "print(\"Wavefn output:\", sim_wave_output)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(20, 16))\n",
    "\n",
    "# Plot for each sampling method\n",
    "for i, (method, output) in enumerate(outputs.items(), 1):\n",
    "    plt.subplot(3, 2, i)\n",
    "    plt.plot(output, marker='o', linestyle='-')\n",
    "    plt.title(f'{method.capitalize()} Sampling Output')\n",
    "    plt.xlabel('Step')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.ylim(0, 9)\n",
    "\n",
    "# Plot for the Wave Function Simulation Output\n",
    "plt.subplot(3, 2, 6)\n",
    "plt.plot(sim_wave_output, marker='o', linestyle='-')\n",
    "plt.title('Wave Function Simulation Output')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.ylim(0, 9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Add heatmap of token probabilities\n",
    "plt.subplot(3, 2, 5)\n",
    "sns.heatmap(probs[0].cpu().numpy(), cmap='viridis')\n",
    "plt.title('Token Probability Heatmap')\n",
    "plt.xlabel('Token')\n",
    "plt.ylabel('Position')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
