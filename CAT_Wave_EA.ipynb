{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from deap import base, creator, tools, algorithms\n",
    "import random\n",
    "\n",
    "# Assuming these are defined in your original code\n",
    "from wavefn import WaveFunction\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 8\n",
    "block_size = 16\n",
    "max_iter = 1000\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "vocab_size = 12  # Assuming this is correct based on your original tokens\n",
    "\n",
    "tokens = ['0','1','2','3','4','5','6','7','8','9','s','e']\n",
    "stoi = { ch:i for i, ch in enumerate(tokens)}\n",
    "itos = { i:ch for i, ch in enumerate(tokens)}\n",
    "enc = lambda s: [stoi[str(c)] for c in s]\n",
    "dec = lambda l: ''.join([itos[i] for i in l[1:-1]])  # Skipping the first and last items ('s' and 'e')\n",
    "\n",
    "# Wave generation functions (from your original code)\n",
    "def generate_random_wave_profile(size):\n",
    "    return ((np.random.rand(size)*10).astype(int))\n",
    "\n",
    "def simulate_wave_sequence(batch_size, size):\n",
    "    initial_profiles = [generate_random_wave_profile(size) for _ in range(batch_size)]\n",
    "    final_profiles = []\n",
    "    for profile in initial_profiles:\n",
    "        transformed_profile = wave_fn.simulate_wave_equation(profile)\n",
    "        transformed_profile_clipped = np.clip(transformed_profile, 0, 9).astype(int)\n",
    "        transformed_profile_as_str = 's' + ''.join(map(str, transformed_profile_clipped.tolist())) + 'e'\n",
    "        final_profiles.append(transformed_profile_as_str)\n",
    "    return initial_profiles, final_profiles\n",
    "\n",
    "def get_batch(batch_size, size, block_size):\n",
    "    initial_profiles, final_profiles = simulate_wave_sequence(batch_size, size)\n",
    "    X = torch.tensor([enc(profile)[:block_size] for profile in initial_profiles], dtype=torch.long)\n",
    "    Y = torch.tensor([enc(profile)[:block_size] for profile in final_profiles], dtype=torch.long)\n",
    "    return X.to(device), Y.to(device)\n",
    "\n",
    "# Simplified model components\n",
    "class CoPEPositionalEmbedding(nn.Module):\n",
    "    def __init__(self, n_embed, max_len):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(n_embed, n_embed)\n",
    "\n",
    "    def forward(self, tok_emb):\n",
    "        seq_len = tok_emb.size(1)\n",
    "        position_indices = torch.arange(seq_len, device=tok_emb.device).unsqueeze(0).repeat(tok_emb.size(0), 1)\n",
    "        position_encoded = self.fc(tok_emb + self.fc(position_indices.float().unsqueeze(-1).expand(-1, -1, tok_emb.size(-1))))\n",
    "        return position_encoded\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embed, n_head):\n",
    "        super().__init__()\n",
    "        self.sa = nn.MultiheadAttention(n_embed, n_head)\n",
    "        self.ffwd = nn.Sequential(\n",
    "            nn.Linear(n_embed, 4 * n_embed),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * n_embed, n_embed)\n",
    "        )\n",
    "        self.ln1 = nn.LayerNorm(n_embed)\n",
    "        self.ln2 = nn.LayerNorm(n_embed)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x), self.ln1(x), self.ln1(x))[0]\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class SmallLLM(nn.Module):\n",
    "    def __init__(self, vocab_size, n_embed, n_head, n_layer):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, n_embed)\n",
    "        self.position_embedding = CoPEPositionalEmbedding(n_embed, block_size)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embed, n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embed)\n",
    "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
    "\n",
    "    def forward(self, idx):\n",
    "        tok_emb = self.token_embedding(idx)\n",
    "        pos_emb = self.position_embedding(tok_emb)\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "        return logits\n",
    "\n",
    "class LLMEnsemble(nn.Module):\n",
    "    def __init__(self, models):\n",
    "        super().__init__()\n",
    "        self.models = nn.ModuleList(models)\n",
    "        self.weights = nn.Parameter(torch.ones(len(models)) / len(models))\n",
    "\n",
    "    def forward(self, idx):\n",
    "        outputs = [model(idx) for model in self.models]\n",
    "        weighted_sum = sum(w * out for w, out in zip(self.weights, outputs))\n",
    "        return weighted_sum\n",
    "\n",
    "# Evolutionary Algorithm setup\n",
    "creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
    "creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
    "\n",
    "def generate_random_architecture():\n",
    "    return {\n",
    "        'n_embed': random.choice([32, 64, 128]),\n",
    "        'n_head': random.choice([2, 4, 8]),\n",
    "        'n_layer': random.choice([2, 4, 6])\n",
    "    }\n",
    "\n",
    "def mutate(individual):\n",
    "    idx = random.randint(0, len(individual) - 1)\n",
    "    individual[idx] = generate_random_architecture()\n",
    "    return individual,\n",
    "\n",
    "def mate(ind1, ind2):\n",
    "    return tools.cxTwoPoint(ind1, ind2)\n",
    "\n",
    "def evaluate(individual):\n",
    "    models = [SmallLLM(vocab_size, **arch) for arch in individual]\n",
    "    ensemble = LLMEnsemble(models).to(device)\n",
    "    optimizer = torch.optim.AdamW(ensemble.parameters(), lr=1e-4)\n",
    "    \n",
    "    total_loss = 0\n",
    "    for _ in range(10):  # Short training\n",
    "        xb, yb = get_batch(batch_size, 64, block_size)\n",
    "        logits = ensemble(xb)\n",
    "        loss = F.cross_entropy(logits.view(-1, vocab_size), yb.view(-1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return (1 / (total_loss / 10),)  # Return fitness (higher is better)\n",
    "\n",
    "toolbox = base.Toolbox()\n",
    "toolbox.register(\"individual\", tools.initRepeat, creator.Individual, generate_random_architecture, n=5)\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "toolbox.register(\"mate\", mate)\n",
    "toolbox.register(\"mutate\", mutate)\n",
    "toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
    "toolbox.register(\"evaluate\", evaluate)\n",
    "\n",
    "def train_best_ensemble(best_individual, epochs=5):\n",
    "    models = [SmallLLM(vocab_size, **arch) for arch in best_individual]\n",
    "    ensemble = LLMEnsemble(models).to(device)\n",
    "    optimizer = torch.optim.AdamW(ensemble.parameters(), lr=1e-4)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=2, verbose=True)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for iter in range(max_iter // epochs):\n",
    "            xb, yb = get_batch(batch_size, 64, block_size)\n",
    "            logits = ensemble(xb)\n",
    "            loss = F.cross_entropy(logits.view(-1, vocab_size), yb.view(-1))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / (max_iter // epochs)\n",
    "        print(f\"Epoch {epoch+1}, Average Loss: {avg_loss}\")\n",
    "        scheduler.step(avg_loss)\n",
    "\n",
    "    return ensemble\n",
    "\n",
    "def main():\n",
    "    pop = toolbox.population(n=50)\n",
    "    hof = tools.HallOfFame(1)\n",
    "    stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "    stats.register(\"avg\", np.mean)\n",
    "    stats.register(\"min\", np.min)\n",
    "    stats.register(\"max\", np.max)\n",
    "\n",
    "    pop, logbook = algorithms.eaSimple(pop, toolbox, cxpb=0.7, mutpb=0.2, ngen=10, stats=stats, halloffame=hof, verbose=True)\n",
    "\n",
    "    print(\"Evolution complete\")\n",
    "    best_individual = hof[0]\n",
    "    print(\"Best individual:\", best_individual)\n",
    "\n",
    "    print(\"Training best ensemble...\")\n",
    "    best_ensemble = train_best_ensemble(best_individual)\n",
    "    \n",
    "    print(\"Training complete\")\n",
    "    return best_ensemble\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize WaveFunction (assuming this is from your original code)\n",
    "    wave_length = 32\n",
    "    c = 1.0\n",
    "    dx = 0.1\n",
    "    dt = 0.1\n",
    "    wave_fn = WaveFunction(c, dx, dt)\n",
    "\n",
    "    # Run the evolutionary process and train the best ensemble\n",
    "    best_model = main()\n",
    "\n",
    "    # Save the model\n",
    "    torch.save(best_model.state_dict(), 'models/best_evolutionary_ensemble.pth')\n",
    "    print(\"Model saved as 'best_evolutionary_ensemble.pth'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "# Define the model architecture (use the best individual architecture from the evolutionary algorithm)\n",
    "best_individual = [{'n_embed': 128, 'n_head': 4, 'n_layer': 6}, {'n_embed': 128, 'n_head': 4, 'n_layer': 6}, {'n_embed': 128, 'n_head': 4, 'n_layer': 4}, {'n_embed': 128, 'n_head': 8, 'n_layer': 4}, {'n_embed': 128, 'n_head': 4, 'n_layer': 4}]\n",
    "models = [SmallLLM(vocab_size, **arch) for arch in best_individual]\n",
    "model = LLMEnsemble(models).to(device)\n",
    "\n",
    "# Load the state dictionary\n",
    "model.load_state_dict(torch.load('models/best_evolutionary_ensemble.pth'))\n",
    "model.eval()\n",
    "\n",
    "# Generate input\n",
    "input_length = 32\n",
    "input = (np.random.rand(input_length) * 10).astype(int)\n",
    "print(f\"Input is: {input}\")\n",
    "encoded_input = enc(input)\n",
    "\n",
    "# Convert to a tensor and add a batch dimension (assuming your model expects a batch)\n",
    "input_tensor = torch.tensor([encoded_input], dtype=torch.long).to(device)\n",
    "max_new_tokens = input_length  # Define how many new tokens you want to generate\n",
    "\n",
    "# Implement a generate function if not already defined in your model\n",
    "def generate(model, idx, max_new_tokens):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits = model(idx)\n",
    "            next_token = torch.argmax(logits[:, -1, :], dim=-1, keepdim=True)\n",
    "            idx = torch.cat((idx, next_token), dim=1)\n",
    "    return idx\n",
    "\n",
    "# Generate output\n",
    "output_tensor = generate(model, input_tensor, max_new_tokens=max_new_tokens)\n",
    "decoded_output = dec(output_tensor[0].tolist())  # Assuming you're interested in the first item in the batch\n",
    "generated_output = decoded_output[input_length+1:].split('e')[0]\n",
    "sim_wave_output = wave_fn.simulate_wave_equation(input)\n",
    "print(\"Decoded Output:\", decoded_output)\n",
    "print(\"Generated Output:\", generated_output)\n",
    "print(\"Wavefn Output:\", sim_wave_output)\n",
    "\n",
    "# Plotting\n",
    "decoded_values = [int(char) for char in generated_output]\n",
    "\n",
    "print(\"Decoded values: \", decoded_values)\n",
    "global_min = 0\n",
    "global_max = 9\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Plot for the Transformer Model Output\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(decoded_values, marker='o', linestyle='-')\n",
    "plt.title('Transformer Model Output')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.ylim(global_min, global_max + 1)  # Set Y-axis limits to be the same for both plots\n",
    "\n",
    "# Plot for the Wave Function Simulation Output\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(sim_wave_output, marker='o', linestyle='-')\n",
    "plt.title('Wave Function Simulation Output')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Tokens')\n",
    "plt.ylim(global_min, global_max + 1)  # Set Y-axis limits to be the same for both plots\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
