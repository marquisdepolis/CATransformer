{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gen\tnevals\tavg    \tmin    \tmax    \n",
      "0  \t50    \t9.77963\t7.57076\t11.2584\n",
      "1  \t46    \t10.1624\t9.07824\t11.4609\n",
      "2  \t39    \t10.2907\t8.78775\t11.5995\n",
      "3  \t39    \t10.1448\t7.72831\t11.5995\n",
      "4  \t38    \t10.2622\t7.3944 \t11.677 \n",
      "5  \t35    \t10.3924\t9.14172\t11.4285\n",
      "6  \t33    \t10.4801\t8.58929\t11.4285\n",
      "7  \t35    \t10.5924\t8.86706\t11.7142\n",
      "8  \t41    \t10.404 \t8.72575\t11.4285\n",
      "9  \t39    \t10.5419\t8.64953\t11.6083\n",
      "10 \t38    \t10.5537\t9.23896\t11.7143\n",
      "Evolution complete\n",
      "Best individual: [{'n_embed': 160, 'n_head': 8, 'n_layer': 1}, {'n_embed': 144, 'n_head': 2, 'n_layer': 8}, {'n_embed': 128, 'n_head': 8, 'n_layer': 8}, {'n_embed': 80, 'n_head': 4, 'n_layer': 1}, {'n_embed': 128, 'n_head': 8, 'n_layer': 1}]\n",
      "Training best ensemble...\n",
      "Epoch 0, Iteration 0: Training Loss = 0.11854824423789978, Validation Loss = 0.1314772218465805\n",
      "Epoch 0, Iteration 500: Training Loss = -0.5976729393005371, Validation Loss = 0.07188022881746292\n",
      "Epoch 1, Average Loss: -0.619447575300932\n",
      "Epoch 1, Iteration 0: Training Loss = -1.2869188785552979, Validation Loss = 0.07435692101716995\n",
      "Epoch 1, Iteration 500: Training Loss = -2.044240951538086, Validation Loss = 0.07198215276002884\n",
      "Epoch 2, Average Loss: -2.0522693036794664\n",
      "Epoch 2, Iteration 0: Training Loss = -2.8466286659240723, Validation Loss = 0.07229611277580261\n",
      "Epoch 2, Iteration 500: Training Loss = -3.6783947944641113, Validation Loss = 0.07194451987743378\n",
      "Epoch 3, Average Loss: -3.6743962466716766\n",
      "Epoch 3, Iteration 0: Training Loss = -4.503837585449219, Validation Loss = 0.07178458571434021\n",
      "Epoch 3, Iteration 500: Training Loss = -5.377508640289307, Validation Loss = 0.0714678019285202\n",
      "Epoch 4, Average Loss: -5.372747208118438\n",
      "Epoch 4, Iteration 0: Training Loss = -6.265014171600342, Validation Loss = 0.07183211296796799\n",
      "Epoch 4, Iteration 500: Training Loss = -7.156485557556152, Validation Loss = 0.07148709893226624\n",
      "Epoch 5, Average Loss: -7.1818318347930905\n",
      "Training complete\n",
      "Model saved as 'best_evolutionary_ensemble.pth'\n"
     ]
    }
   ],
   "source": [
    "# transformer plus evolutionary algorithms: learning some wave rules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from deap import base, creator, tools, algorithms\n",
    "import random\n",
    "\n",
    "from wavefn import WaveFunction\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 8\n",
    "block_size = 16\n",
    "max_iter = 5000\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "vocab_size = 12  # Assuming this is correct based on your original tokens\n",
    "\n",
    "tokens = ['0','1','2','3','4','5','6','7','8','9','s','e']\n",
    "stoi = { ch:i for i, ch in enumerate(tokens)}\n",
    "itos = { i:ch for i, ch in enumerate(tokens)}\n",
    "enc = lambda s: [stoi[str(c)] for c in s]\n",
    "dec = lambda l: ''.join([itos[i] for i in l[1:-1]])  # Skipping the first and last items ('s' and 'e')\n",
    "\n",
    "# Wave generation functions (from your original code)\n",
    "def generate_random_wave_profile(size):\n",
    "    return ((np.random.rand(size)*10).astype(int))\n",
    "\n",
    "def simulate_wave_sequence(batch_size, size):\n",
    "    initial_profiles = [generate_random_wave_profile(size) for _ in range(batch_size)]\n",
    "    final_profiles = []\n",
    "    for profile in initial_profiles:\n",
    "        transformed_profile = wave_fn.simulate_wave_equation(profile)\n",
    "        # Remove clipping to allow for more variation\n",
    "        transformed_profile_as_str = 's' + ''.join(map(lambda x: str(int(x) % 10), transformed_profile.tolist())) + 'e'\n",
    "        final_profiles.append(transformed_profile_as_str)\n",
    "    return initial_profiles, final_profiles\n",
    "\n",
    "def get_batch(batch_size, size, block_size):\n",
    "    initial_profiles, final_profiles = simulate_wave_sequence(batch_size, size)\n",
    "    X = torch.tensor([enc('s' + ''.join(map(str, profile)) + 'e')[:block_size] for profile in initial_profiles], dtype=torch.long)\n",
    "    Y = torch.tensor([enc(profile)[:block_size] for profile in final_profiles], dtype=torch.long)\n",
    "    return X.to(device), Y.to(device)\n",
    "\n",
    "def decode_model_output(logits):\n",
    "    # Convert logits to probabilities and get the most likely token\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    predicted_tokens = torch.argmax(probs, dim=-1)\n",
    "    \n",
    "    # Decode the predicted tokens\n",
    "    decoded_outputs = []\n",
    "    for sequence in predicted_tokens:\n",
    "        decoded_sequence = dec(sequence.tolist())\n",
    "        decoded_outputs.append(decoded_sequence)\n",
    "    \n",
    "    return decoded_outputs\n",
    "\n",
    "class CoPEPositionalEmbedding(nn.Module):\n",
    "    def __init__(self, n_embed, max_len):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(n_embed, n_embed)\n",
    "\n",
    "    def forward(self, tok_emb):\n",
    "        seq_len = tok_emb.size(1)\n",
    "        position_indices = torch.arange(seq_len, device=tok_emb.device).unsqueeze(0).repeat(tok_emb.size(0), 1)\n",
    "        position_encoded = self.fc(tok_emb + self.fc(position_indices.float().unsqueeze(-1).expand(-1, -1, tok_emb.size(-1))))\n",
    "        return position_encoded\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embed, n_head):\n",
    "        super().__init__()\n",
    "        self.sa = nn.MultiheadAttention(n_embed, n_head)\n",
    "        self.ffwd = nn.Sequential(\n",
    "            nn.Linear(n_embed, 4 * n_embed),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * n_embed, n_embed)\n",
    "        )\n",
    "        self.ln1 = nn.LayerNorm(n_embed)\n",
    "        self.ln2 = nn.LayerNorm(n_embed)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x), self.ln1(x), self.ln1(x))[0]\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class SmallLLM(nn.Module):\n",
    "    def __init__(self, vocab_size, n_embed, n_head, n_layer):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, n_embed)\n",
    "        self.position_embedding = CoPEPositionalEmbedding(n_embed, block_size)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embed, n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embed)\n",
    "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
    "\n",
    "    def forward(self, idx):\n",
    "        tok_emb = self.token_embedding(idx)\n",
    "        pos_emb = self.position_embedding(tok_emb)\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "        return logits\n",
    "\n",
    "class LLMEnsemble(nn.Module):\n",
    "    def __init__(self, models):\n",
    "        super().__init__()\n",
    "        self.models = nn.ModuleList(models)\n",
    "        self.weights = nn.Parameter(torch.ones(len(models)) / len(models))\n",
    "\n",
    "    def forward(self, idx):\n",
    "        outputs = [model(idx) for model in self.models]\n",
    "        weighted_sum = sum(w * out for w, out in zip(self.weights, outputs))\n",
    "        return weighted_sum\n",
    "\n",
    "# Evolutionary Algorithm setup\n",
    "creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
    "creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
    "\n",
    "def generate_random_architecture():\n",
    "    return {\n",
    "        'n_embed': random.choice([32, 48, 64, 80, 96, 112, 128, 144, 160]),\n",
    "        'n_head': random.choice([1, 2, 4, 8]),\n",
    "        'n_layer': random.choice([1, 2, 4, 8])\n",
    "    }\n",
    "\n",
    "def mutate(individual):\n",
    "    idx = random.randint(0, len(individual) - 1)\n",
    "    individual[idx] = generate_random_architecture()\n",
    "    return individual,\n",
    "\n",
    "def mate(ind1, ind2):\n",
    "    return tools.cxTwoPoint(ind1, ind2)\n",
    "\n",
    "def evaluate(individual):\n",
    "    models = [SmallLLM(vocab_size, **arch) for arch in individual]\n",
    "    ensemble = LLMEnsemble(models).to(device)\n",
    "    optimizer = torch.optim.AdamW(ensemble.parameters(), lr=1e-4)\n",
    "    \n",
    "    total_loss = 0\n",
    "    for _ in range(10):  # Short training\n",
    "        xb, yb = get_batch(batch_size, 64, block_size)\n",
    "        logits = ensemble(xb)\n",
    "        logits_flat = logits.view(-1, vocab_size).float()  # Flatten logits for mse\n",
    "        yb_flat = F.one_hot(yb.view(-1), num_classes=vocab_size).float()  # Flatten and one-hot encode targets for mse\n",
    "        loss = F.mse_loss(logits_flat, yb_flat)\n",
    "        # loss = F.cross_entropy(logits.view(-1, vocab_size), yb.view(-1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return (1 / (total_loss / 10),)  # Return fitness (higher is better)\n",
    "\n",
    "toolbox = base.Toolbox()\n",
    "toolbox.register(\"individual\", tools.initRepeat, creator.Individual, generate_random_architecture, n=5)\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "toolbox.register(\"mate\", mate)\n",
    "toolbox.register(\"mutate\", mutate)\n",
    "toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
    "toolbox.register(\"evaluate\", evaluate)\n",
    "\n",
    "def custom_loss(predictions, targets, vocab_size):\n",
    "    # ce_loss = F.cross_entropy(predictions.view(-1, vocab_size), targets.view(-1))\n",
    "    # zero_penalty = torch.mean(torch.abs(predictions[:, :, 0]))  # Penalize predicting zero\n",
    "    # return ce_loss + 0.1 * zero_penalty\n",
    "    predictions_flat = predictions.view(-1, vocab_size).float()  # Flatten predictions\n",
    "    targets_flat = F.one_hot(targets.view(-1), num_classes=vocab_size).float()  # Flatten and one-hot encode targets\n",
    "    mse_loss = F.mse_loss(predictions_flat, targets_flat)\n",
    "    zero_penalty = torch.mean(torch.abs(predictions[:, :, 0]))  # Penalize predicting zero\n",
    "    return mse_loss + 0.1 * zero_penalty\n",
    "\n",
    "def diversity_loss(ensemble_outputs):\n",
    "    mean_output = torch.mean(ensemble_outputs, dim=0)\n",
    "    return -torch.mean(torch.abs(ensemble_outputs - mean_output))\n",
    "\n",
    "def train_best_ensemble(best_individual, epochs=5):\n",
    "    models = [SmallLLM(vocab_size, **arch) for arch in best_individual]\n",
    "    ensemble = LLMEnsemble(models).to(device)\n",
    "    optimizer = torch.optim.AdamW(ensemble.parameters(), lr=1e-4)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=2, verbose=True)\n",
    "\n",
    "    val_batch_size = batch_size\n",
    "    max_norm = 1\n",
    "    eval_interval = 500\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for iter in range(max_iter // epochs):\n",
    "            ensemble.train()\n",
    "            xb, yb = get_batch(batch_size, 64, block_size)\n",
    "            \n",
    "            # Get individual model outputs\n",
    "            individual_outputs = [model(xb) for model in ensemble.models]\n",
    "            \n",
    "            # Compute ensemble output\n",
    "            ensemble_output = ensemble(xb)\n",
    "            \n",
    "            # Compute custom loss\n",
    "            custom_loss_value = custom_loss(ensemble_output, yb, vocab_size)\n",
    "            \n",
    "            # Compute diversity loss\n",
    "            div_loss_value = diversity_loss(torch.stack(individual_outputs))\n",
    "            \n",
    "            # Combine losses\n",
    "            loss = custom_loss_value + 0.1 * div_loss_value  # You can adjust this weight\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(ensemble.parameters(), max_norm)\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            if iter % eval_interval == 0:\n",
    "                ensemble.eval()\n",
    "                with torch.no_grad():\n",
    "                    xv, yv = get_batch(val_batch_size, 64, block_size)\n",
    "                    val_logits = ensemble(xv)\n",
    "                    val_loss = custom_loss(val_logits, yv, vocab_size)\n",
    "                    print(f\"Epoch {epoch}, Iteration {iter}: Training Loss = {loss.item()}, Validation Loss = {val_loss.item()}\")\n",
    "                    \n",
    "                    # Decode and print examples\n",
    "                    # input_decoded = [dec(seq.tolist()) for seq in xv[:3]]\n",
    "                    # target_decoded = [dec(seq.tolist()) for seq in yv[:3]]\n",
    "                    # predicted_decoded = decode_model_output(val_logits[:3])\n",
    "                    \n",
    "                    # print(\"Input examples:\", input_decoded)\n",
    "                    # print(\"Target examples:\", target_decoded)\n",
    "                    # print(\"Predicted examples:\", predicted_decoded)\n",
    "                \n",
    "                ensemble.train()\n",
    "\n",
    "        avg_loss = total_loss / (max_iter // epochs)\n",
    "        print(f\"Epoch {epoch+1}, Average Loss: {avg_loss}\")\n",
    "        scheduler.step(avg_loss)\n",
    "\n",
    "    return ensemble\n",
    "\n",
    "def main():\n",
    "    global best_individual\n",
    "    pop = toolbox.population(n=50)\n",
    "    hof = tools.HallOfFame(1)\n",
    "    stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "    stats.register(\"avg\", np.mean)\n",
    "    stats.register(\"min\", np.min)\n",
    "    stats.register(\"max\", np.max)\n",
    "\n",
    "    pop, logbook = algorithms.eaSimple(pop, toolbox, cxpb=0.7, mutpb=0.2, ngen=10, stats=stats, halloffame=hof, verbose=True)\n",
    "\n",
    "    print(\"Evolution complete\")\n",
    "    best_individual = hof[0]\n",
    "    print(\"Best individual:\", best_individual)\n",
    "\n",
    "    print(\"Training best ensemble...\")\n",
    "    best_ensemble = train_best_ensemble(best_individual)\n",
    "    \n",
    "    print(\"Training complete\")\n",
    "    return best_ensemble\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize WaveFunction (assuming this is from your original code)\n",
    "    wave_length = 16\n",
    "    c = 1.0\n",
    "    dx = 0.1\n",
    "    dt = 0.1\n",
    "    wave_fn = WaveFunction(c, dx, dt)\n",
    "\n",
    "    # Run the evolutionary process and train the best ensemble\n",
    "    best_model = main()\n",
    "\n",
    "    # Save the model\n",
    "    torch.save(best_model.state_dict(), 'models/best_evolutionary_ensemble.pth')\n",
    "    print(\"Model saved as 'best_evolutionary_ensemble.pth'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input wave: [3 9 7 5 1 1 0 8 6 7 0 9 8 2 1 1]\n",
      "Ensemble weights: [0.19386315 0.21416089 0.18795846 0.19152853 0.17795993]\n",
      "Weight distribution: (array([1, 0, 1, 1, 1, 0, 0, 0, 0, 1]), array([0.17795993, 0.18158004, 0.18520012, 0.18882021, 0.19244032,\n",
      "       0.19606042, 0.1996805 , 0.2033006 , 0.2069207 , 0.2105408 ,\n",
      "       0.21416089], dtype=float32))\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'logits' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 77\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnsemble weights:\u001b[39m\u001b[38;5;124m\"\u001b[39m, ensemble_weights)\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWeight distribution:\u001b[39m\u001b[38;5;124m\"\u001b[39m, np\u001b[38;5;241m.\u001b[39mhistogram(ensemble_weights))\n\u001b[0;32m---> 77\u001b[0m analyze_token_distribution(\u001b[43mlogits\u001b[49m)\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m# Simulate wave using the original input\u001b[39;00m\n\u001b[1;32m     80\u001b[0m sim_wave_output \u001b[38;5;241m=\u001b[39m wave_fn\u001b[38;5;241m.\u001b[39msimulate_wave_equation(input_wave)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'logits' is not defined"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# Sampling functions\n",
    "def temperature_sampling(logits, temperature=0.7):  # Lowered from 1.0\n",
    "    probs = F.softmax(logits / temperature, dim=-1)\n",
    "    return torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "def calculate_accuracy(predicted, ground_truth):\n",
    "    correct = sum(p == g for p, g in zip(predicted, ground_truth))\n",
    "    return correct / len(ground_truth)\n",
    "\n",
    "def top_k_sampling(logits, k=50):\n",
    "    top_k = torch.topk(logits, k)\n",
    "    indices_to_remove = logits < top_k.values[..., -1, None]\n",
    "    logits[indices_to_remove] = float('-inf')\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    return torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "def nucleus_sampling(logits, p=0.9):\n",
    "    sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "    cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "    sorted_indices_to_remove = cumulative_probs > p\n",
    "    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "    sorted_indices_to_remove[..., 0] = 0\n",
    "    indices_to_remove = sorted_indices_to_remove.scatter(dim=1, index=sorted_indices, src=sorted_indices_to_remove)\n",
    "    logits[indices_to_remove] = float('-inf')\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    return torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "def generate_output(model, input_tensor, max_length=18, sampling_method='temperature', **kwargs):\n",
    "    generated = input_tensor\n",
    "    for _ in range(max_length - input_tensor.size(1)):\n",
    "        with torch.no_grad():\n",
    "            logits = model(generated)[:, -1, :]\n",
    "            if sampling_method == 'temperature':\n",
    "                next_token = temperature_sampling(logits, temperature=kwargs.get('temperature', 0.8))\n",
    "            elif sampling_method == 'top_k':\n",
    "                next_token = top_k_sampling(logits, k=kwargs.get('k', 50))\n",
    "            elif sampling_method == 'nucleus':\n",
    "                next_token = nucleus_sampling(logits, p=kwargs.get('p', 0.9))\n",
    "            else:\n",
    "                next_token = torch.argmax(logits, dim=-1).unsqueeze(0)\n",
    "        generated = torch.cat((generated, next_token), dim=1)\n",
    "    return generated\n",
    "\n",
    "# Load the model (assuming this part is correct)\n",
    "models = [SmallLLM(vocab_size, **arch) for arch in best_individual]\n",
    "model = LLMEnsemble(models).to(device)\n",
    "model.load_state_dict(torch.load('models/best_evolutionary_ensemble.pth'))\n",
    "model.eval()\n",
    "\n",
    "# Generate input\n",
    "np.random.seed(42)  # Seed for reproducibility\n",
    "input_length = 16\n",
    "input_wave = generate_random_wave_profile(input_length)\n",
    "print(f\"Input wave: {input_wave}\")\n",
    "\n",
    "# Encode input with start and end tokens\n",
    "encoded_input = enc('s' + ''.join(map(str, input_wave)) + 'e')\n",
    "input_tensor = torch.tensor([encoded_input], dtype=torch.long).to(device)\n",
    "        \n",
    "def analyze_token_distribution(logits):\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    token_dist = probs.mean(dim=(0, 1)).cpu().numpy()\n",
    "    print(\"Token distribution:\")\n",
    "    for token, prob in sorted(zip(itos.values(), token_dist), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"  {token}: {prob:.4f}\")\n",
    "\n",
    "# After loading the model\n",
    "ensemble_weights = model.weights.detach().cpu().numpy()\n",
    "print(\"Ensemble weights:\", ensemble_weights)\n",
    "print(\"Weight distribution:\", np.histogram(ensemble_weights))\n",
    "analyze_token_distribution(logits)\n",
    "\n",
    "# Simulate wave using the original input\n",
    "sim_wave_output = wave_fn.simulate_wave_equation(input_wave)\n",
    "print(\"Wavefn output:\", sim_wave_output)\n",
    "\n",
    "# Generate output using different sampling methods\n",
    "sampling_methods = ['argmax', 'temperature', 'top_k', 'nucleus']\n",
    "outputs = {}\n",
    "\n",
    "for method in sampling_methods:\n",
    "    with torch.no_grad():\n",
    "        if method == 'argmax':\n",
    "            logits = model(input_tensor)\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            predicted_tokens = torch.argmax(probs, dim=-1)\n",
    "        else:\n",
    "            predicted_tokens = generate_output(model, input_tensor, sampling_method=method)\n",
    "\n",
    "        print(f\"\\nSampling method: {method}\")\n",
    "        print(\"Logits shape:\", logits.shape)\n",
    "        print(\"Logits mean:\", logits.mean().item())\n",
    "        print(\"Logits std:\", logits.std().item())\n",
    "\n",
    "        # Print probabilities for the first few positions\n",
    "        for i in range(5):\n",
    "            print(f\"Top 5 probable tokens at position {i}:\")\n",
    "            top_probs, top_indices = torch.topk(probs[0, i], 5)\n",
    "            for prob, idx in zip(top_probs, top_indices):\n",
    "                print(f\"  Token: {itos[idx.item()]}, Probability: {prob.item():.4f}\")\n",
    "\n",
    "        # Decode the entire output, including start and end tokens\n",
    "        decoded_output = ''.join([itos[i] for i in predicted_tokens[0].tolist()])\n",
    "        print(\"Raw decoded output:\", decoded_output)\n",
    "\n",
    "        # Extract the actual wave values (remove 's' and 'e', and convert to integers)\n",
    "        generated_output = [int(char) for char in decoded_output[1:-1] if char.isdigit()]\n",
    "        print(\"Generated output:\", generated_output)\n",
    "        \n",
    "        # Calculate and print accuracy\n",
    "        accuracy = calculate_accuracy(generated_output, sim_wave_output)\n",
    "        print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "        outputs[method] = generated_output\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(20, 16))\n",
    "\n",
    "# Plot for each sampling method\n",
    "for i, (method, output) in enumerate(outputs.items(), 1):\n",
    "    plt.subplot(3, 2, i)\n",
    "    plt.plot(output, marker='o', linestyle='-')\n",
    "    plt.title(f'{method.capitalize()} Sampling Output')\n",
    "    plt.xlabel('Step')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.ylim(0, 9)\n",
    "\n",
    "# Plot for the Wave Function Simulation Output\n",
    "plt.subplot(3, 2, 6)\n",
    "plt.plot(sim_wave_output, marker='o', linestyle='-')\n",
    "plt.title('Wave Function Simulation Output')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.ylim(0, 9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Add heatmap of token probabilities\n",
    "plt.subplot(3, 2, 5)\n",
    "sns.heatmap(probs[0].cpu().numpy(), cmap='viridis')\n",
    "plt.title('Token Probability Heatmap')\n",
    "plt.xlabel('Token')\n",
    "plt.ylabel('Position')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
