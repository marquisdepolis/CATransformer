{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Iteration 0: Training Loss = 0.6952833533287048, Validation Loss = 0.695400595664978\n",
      "Epoch 1, Iteration 0: Training Loss = 0.6653444170951843, Validation Loss = 0.6780953407287598\n",
      "Epoch 2, Iteration 0: Training Loss = 0.6726760268211365, Validation Loss = 0.6694046258926392\n",
      "Epoch 3, Iteration 0: Training Loss = 0.6699610948562622, Validation Loss = 0.6691824197769165\n",
      "Epoch 4, Iteration 0: Training Loss = 0.6654856204986572, Validation Loss = 0.6781882643699646\n",
      "Epoch 5, Iteration 0: Training Loss = 0.6764615774154663, Validation Loss = 0.676921010017395\n",
      "Epoch 00006: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 6, Iteration 0: Training Loss = 0.6737993955612183, Validation Loss = 0.6736050844192505\n",
      "Epoch 7, Iteration 0: Training Loss = 0.6760389804840088, Validation Loss = 0.6707116961479187\n",
      "Epoch 00008: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 8, Iteration 0: Training Loss = 0.6611474752426147, Validation Loss = 0.6747640371322632\n",
      "Epoch 9, Iteration 0: Training Loss = 0.6714613437652588, Validation Loss = 0.6635408997535706\n"
     ]
    }
   ],
   "source": [
    "# transformer plus cellular automata: conway game of life\n",
    "import torch\n",
    "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn \n",
    "from torch.nn import functional as F\n",
    "from gameoflife import GameOfLife\n",
    "\n",
    "batch_size = 8\n",
    "block_size = 256 # Has to be the square of grid_size to read the full grid\n",
    "max_iter = 5000\n",
    "epochs = 10\n",
    "eval_interval = 500\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 100\n",
    "n_embed = 64\n",
    "n_head = 8\n",
    "n_layer = 16\n",
    "dropout = 0\n",
    "text = []\n",
    "\n",
    "# create dictionaries and then define unique characters for encoding and decoding\n",
    "tokens = ['0', '1'] #, 's', 'e']\n",
    "\n",
    "# Example usage with cellular automata\n",
    "grid_size = 16  # Grid size for the cellular automata\n",
    "step_count = 1  # Number of steps to evolve the cellular automata\n",
    "\n",
    "vocab_size=len(tokens)\n",
    "stoi = { ch:i for i, ch in enumerate(tokens)}\n",
    "itos = { i:ch for i, ch in enumerate(tokens)}\n",
    "enc = lambda s: [stoi[c] for c in s]\n",
    "dec = lambda l: ''.join([itos[i] for i in l])\n",
    "# enc = lambda s: [stoi['s']] + [stoi[c] for c in s] + [stoi['e']]\n",
    "# dec = lambda l: ''.join([itos[i] for i in l[1:-1]])\n",
    "\n",
    "def generate_random_input_string(size):\n",
    "    \"\"\"Generate a random grid as a string for a given grid size.\"\"\"\n",
    "    return ''.join(np.random.choice(tokens[:2], size*size))\n",
    "\n",
    "def generate_game_of_life_sequence(batch_size, grid_size, step_count):\n",
    "    \"\"\"Generate a batch of Game of Life initial states and final states.\"\"\"\n",
    "    initial_states = [generate_random_input_string(grid_size) for _ in range(batch_size)]\n",
    "    final_states = []\n",
    "    for state in initial_states:\n",
    "        # Directly use state without 's' and 'e' tokens for simulation\n",
    "        game = GameOfLife(input_string=state, generations=step_count)\n",
    "        final_state = game.run_simulation()  # Assuming this returns the final state string\n",
    "        # Only add 's' and 'e' tokens for neural network processing, not for GameOfLife simulation\n",
    "        # final_state_with_tokens = 's' + final_state + 'e'\n",
    "        final_states.append(final_state)\n",
    "    # Add 's' and 'e' tokens to initial states after simulation to maintain consistency\n",
    "    initial_states_with_tokens = [state for state in initial_states]\n",
    "    return initial_states_with_tokens, final_states\n",
    "\n",
    "# Define an appropriate size for your validation batch\n",
    "val_batch_size = 20  \n",
    "\n",
    "# load data\n",
    "def get_batch(batch_size, grid_size, step_count, block_size):\n",
    "    initial_states, final_states = generate_game_of_life_sequence(batch_size, grid_size, step_count)\n",
    "    X = torch.tensor([enc(s)[:block_size] for s in initial_states], dtype=torch.long)\n",
    "    Y = torch.tensor([enc(s)[:block_size] for s in final_states], dtype=torch.long)\n",
    "    return X.to(device), Y.to(device)\n",
    "\n",
    "# single head attention\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embed,head_size,bias=False)\n",
    "        self.query = nn.Linear(n_embed,head_size,bias=False)\n",
    "        self.value = nn.Linear(n_embed,head_size,bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size,block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        wei = q @ k.transpose(-2,-1) *C**-0.5 # scaled attention\n",
    "        # wei = wei.masked_fill(self.tril[:T,:T]==0,float('-inf')) # decoder block\n",
    "        wei = F.softmax(wei,dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "        v = self.value(x)\n",
    "        out = wei@v\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embed,n_embed)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,x):\n",
    "        out =  torch.cat([h(x) for h in self.heads], dim = -1)\n",
    "        out = self.proj(out) # Projection si the linear transformation of the outcome of prev layer\n",
    "        return out\n",
    "\n",
    "class SinusoidalActivation(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return torch.sin(x)\n",
    "        # return x + torch.sin(x) ** 2\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embed):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embed,4* n_embed), \n",
    "            # nn.ReLU(),\n",
    "            nn.GELU(),\n",
    "            # SinusoidalActivation(),\n",
    "            nn.Linear(4* n_embed, n_embed),\n",
    "            nn.Dropout(dropout),\n",
    "            )\n",
    "        self\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embed, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embed //n_head\n",
    "        self.sa = MultiHeadAttention(n_head,head_size)\n",
    "        self.ffwd = FeedForward(n_embed)\n",
    "        self.ln1 = nn.LayerNorm(n_embed)\n",
    "        self.ln2 = nn.LayerNorm(n_embed)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = x + self.sa(self.ln1(x)) # add x for residual connections\n",
    "        x = x + self.ffwd(self.ln1(x))\n",
    "        return x\n",
    "\n",
    "# bigram language model\n",
    "class LanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
    "        self.position_embedding_table = nn.Embedding(block_size,n_embed)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embed,n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embed)\n",
    "        self.lm_head = nn.Linear(n_embed,vocab_size)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def lossy(self, output, target):\n",
    "        output = output.view(target.shape)  # Reshape output to match target's shape\n",
    "        return torch.mean((output != target).float())\n",
    "\n",
    "    def hamming_loss(self, predictions, targets):\n",
    "        # Ensure predictions are binary (0 or 1)\n",
    "        predictions = predictions > 0.5\n",
    "        # Calculate mismatches\n",
    "        mismatches = torch.ne(predictions.float(), targets.float()).float()\n",
    "        # Calculate the average number of mismatches\n",
    "        return torch.mean(mismatches)\n",
    "\n",
    "    def forward(self,idx,targets=None):\n",
    "        B,T = idx.shape\n",
    "        tok_emb = self.token_embedding_table(idx)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device = device))\n",
    "        x = tok_emb+pos_emb\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        # print(f\"logits are shape {logits.shape} are: {logits} for idx: {idx}\")\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(-1, vocab_size)  # Reshape logits to [batch_size * block_size, vocab_size]\n",
    "            targets = targets.view(-1,1)  # Flatten targets to [batch_size * block_size]\n",
    "            targets = torch.cat((1 - targets, targets), dim=1)  \n",
    "            # loss = self.lossy(logits, targets)\n",
    "            # loss = F.cross_entropy(logits, targets)\n",
    "            # predictions = torch.sigmoid(logits) > 0.5\n",
    "            # loss = self.hamming_loss(predictions.view(-1), targets.view(-1))\n",
    "            loss = F.binary_cross_entropy_with_logits(logits, targets.float(), reduction='mean')\n",
    "            # loss = F.mse_loss(logits, F.one_hot(targets, num_classes=vocab_size).float())\n",
    "            # print(f\"logits are shape {logits.shape} are: {loss} for idx: {idx}\")\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self,idx,max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, loss = self(idx_cond)\n",
    "            logits = logits[:,-1,:]\n",
    "            probs = F.softmax(logits,dim=-1)\n",
    "            idx_next = torch.multinomial(probs,num_samples=1)\n",
    "            idx=torch.cat((idx, idx_next), dim = 1)\n",
    "        return idx\n",
    "\n",
    "model = LanguageModel()\n",
    "m = model.to(device)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "# optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate, alpha=0.99, eps=1e-08, weight_decay=0.01, momentum=0.5, centered=False)\n",
    "optimizer = torch.optim.Adadelta(model.parameters(), lr=learning_rate, rho=0.9, eps=1e-06, weight_decay=0.01)\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "# scheduler = StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.2, patience=1, verbose=True)\n",
    "loss = None  # Initialize loss variable outside the loop\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for iter in range(max_iter // epochs):  # Distribute iterations across epochs\n",
    "        model.train()\n",
    "        xb, yb = get_batch(batch_size, grid_size, step_count, block_size)\n",
    "        logits, loss = model(xb, yb)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        max_norm = 0.9\n",
    "        # Clip gradients to avoid exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
    "        optimizer.step()\n",
    "\n",
    "        if iter % eval_interval == 0 and loss is not None:  # Validation logic\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                xv, yv = get_batch(val_batch_size, grid_size, step_count, block_size)\n",
    "                val_logits, val_loss = model(xv, yv)\n",
    "                print(f\"Epoch {epoch}, Iteration {iter}: Training Loss = {loss.item()}, Validation Loss = {val_loss.item()}\")\n",
    "            model.train()\n",
    "\n",
    "    scheduler.step(val_loss)  # Update the learning rate at the end of each epoch\n",
    "\n",
    "# Save:\n",
    "torch.save(model, 'cat_cgol_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input is: 0011010101000001010110110110101101101000111110111101111010001011\n",
      "Generated from CA is: 0111110001000101010110110100101100001000000001110000000010000000\n",
      "\n",
      "Generated from T is: 1000101111011000011101111010010001000010001101010001111110100010\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "64\n",
      "64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model = torch.load('cat_cgol_model.pth')\n",
    "model.eval()\n",
    "input_length = 8\n",
    "generations = 1\n",
    "input_sequence = ''.join(np.random.choice(tokens, input_length*input_length))\n",
    "print(f\"Input is: {input_sequence}\")\n",
    "context = torch.tensor(enc(input_sequence), dtype=torch.long, device=device).unsqueeze(0)\n",
    "output = model.generate(context, max_new_tokens=len(input_sequence))\n",
    "generated_text_t = dec(output[0].tolist())\n",
    "game = GameOfLife(input_sequence, generations=generations)\n",
    "\n",
    "generated_text_ca = game.run_simulation()\n",
    "\n",
    "def visualize_grid_with_modifiers(grid):\n",
    "    \"\"\"Visualise the grid.\"\"\"\n",
    "    base_colors = {'0': 'red', '1': 'blue', 's': 'grey', 'e': 'black'}\n",
    "    colors = []\n",
    "    for row in grid:\n",
    "        row_colors = [base_colors[base] for base in row]\n",
    "        colors.extend(row_colors)\n",
    "\n",
    "    plt.figure(figsize=(20, 2))\n",
    "    plt.bar(range(len(colors)), np.ones(len(colors)), color=colors)\n",
    "    plt.axis('off')\n",
    "    plt.title('Visualization')\n",
    "    plt.show()\n",
    "\n",
    "print(f\"Generated from CA is: {generated_text_ca}\\n\")\n",
    "print(f\"Generated from T is: {(generated_text_t[len(input_sequence):])}\\n\")\n",
    "visualize_grid_with_modifiers(input_sequence)\n",
    "visualize_grid_with_modifiers(generated_text_ca)\n",
    "visualize_grid_with_modifiers(generated_text_t[len(input_sequence):])  # Use the fixed, split decoded output\n",
    "\n",
    "print(len(generated_text_t))\n",
    "print(len(input_sequence))\n",
    "print(len(generated_text_t)-(len(input_sequence)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
