{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Iteration 0: Training Loss = 0.7355570197105408, Validation Loss = 0.7264975309371948\n",
      "Epoch 1, Iteration 0: Training Loss = 0.677421510219574, Validation Loss = 0.6844095587730408\n",
      "Epoch 2, Iteration 0: Training Loss = 0.6698231101036072, Validation Loss = 0.6695690751075745\n",
      "Epoch 3, Iteration 0: Training Loss = 0.6767109632492065, Validation Loss = 0.6703874468803406\n",
      "Epoch 4, Iteration 0: Training Loss = 0.6759398579597473, Validation Loss = 0.6732644438743591\n",
      "Epoch 00005: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 5, Iteration 0: Training Loss = 0.6737722754478455, Validation Loss = 0.6658201813697815\n",
      "Epoch 6, Iteration 0: Training Loss = 0.6749903559684753, Validation Loss = 0.6683206558227539\n",
      "Epoch 7, Iteration 0: Training Loss = 0.6698014140129089, Validation Loss = 0.6737262606620789\n",
      "Epoch 00008: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 8, Iteration 0: Training Loss = 0.6798834204673767, Validation Loss = 0.673075795173645\n",
      "Epoch 9, Iteration 0: Training Loss = 0.6735895276069641, Validation Loss = 0.6682706475257874\n",
      "Epoch 00010: reducing learning rate of group 0 to 8.0000e-06.\n"
     ]
    }
   ],
   "source": [
    "# transformer plus cellular automata: conway game of life\n",
    "import torch\n",
    "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn \n",
    "from torch.nn import functional as F\n",
    "from gameoflife import GameOfLife\n",
    "\n",
    "batch_size = 8\n",
    "block_size = 256 # Has to be the square of grid_size to read the full grid\n",
    "max_iter = 5000\n",
    "epochs = 10\n",
    "eval_interval = 500\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 100\n",
    "n_embed = 64\n",
    "n_head = 8\n",
    "n_layer = 16\n",
    "dropout = 0\n",
    "text = []\n",
    "\n",
    "# create dictionaries and then define unique characters for encoding and decoding\n",
    "tokens = ['0', '1'] #, 's', 'e']\n",
    "\n",
    "# Example usage with cellular automata\n",
    "grid_size = 16  # Grid size for the cellular automata\n",
    "step_count = 1  # Number of steps to evolve the cellular automata\n",
    "\n",
    "vocab_size=len(tokens)\n",
    "stoi = { ch:i for i, ch in enumerate(tokens)}\n",
    "itos = { i:ch for i, ch in enumerate(tokens)}\n",
    "enc = lambda s: [stoi[c] for c in s]\n",
    "dec = lambda l: ''.join([itos[i] for i in l[1:-1]])\n",
    "# enc = lambda s: [stoi['s']] + [stoi[c] for c in s] + [stoi['e']]\n",
    "# dec = lambda l: ''.join([itos[i] for i in l[1:-1]])\n",
    "\n",
    "def generate_random_input_string(size):\n",
    "    \"\"\"Generate a random grid as a string for a given grid size.\"\"\"\n",
    "    return ''.join(np.random.choice(tokens[:2], size*size))\n",
    "\n",
    "def generate_game_of_life_sequence(batch_size, grid_size, step_count):\n",
    "    \"\"\"Generate a batch of Game of Life initial states and final states.\"\"\"\n",
    "    initial_states = [generate_random_input_string(grid_size) for _ in range(batch_size)]\n",
    "    final_states = []\n",
    "    for state in initial_states:\n",
    "        # Directly use state without 's' and 'e' tokens for simulation\n",
    "        game = GameOfLife(input_string=state, generations=step_count)\n",
    "        final_state = game.run_simulation()  # Assuming this returns the final state string\n",
    "        # Only add 's' and 'e' tokens for neural network processing, not for GameOfLife simulation\n",
    "        # final_state_with_tokens = 's' + final_state + 'e'\n",
    "        final_states.append(final_state)\n",
    "    # Add 's' and 'e' tokens to initial states after simulation to maintain consistency\n",
    "    initial_states_with_tokens = [state for state in initial_states]\n",
    "    return initial_states_with_tokens, final_states\n",
    "\n",
    "# Define an appropriate size for your validation batch\n",
    "val_batch_size = 20  \n",
    "\n",
    "# load data\n",
    "def get_batch(batch_size, grid_size, step_count, block_size):\n",
    "    initial_states, final_states = generate_game_of_life_sequence(batch_size, grid_size, step_count)\n",
    "    X = torch.tensor([enc(s)[:block_size] for s in initial_states], dtype=torch.long)\n",
    "    Y = torch.tensor([enc(s)[:block_size] for s in final_states], dtype=torch.long)\n",
    "    return X.to(device), Y.to(device)\n",
    "\n",
    "# single head attention\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embed,head_size,bias=False)\n",
    "        self.query = nn.Linear(n_embed,head_size,bias=False)\n",
    "        self.value = nn.Linear(n_embed,head_size,bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size,block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        wei = q @ k.transpose(-2,-1) *C**-0.5 # scaled attention\n",
    "        # wei = wei.masked_fill(self.tril[:T,:T]==0,float('-inf')) # decoder block\n",
    "        wei = F.softmax(wei,dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "        v = self.value(x)\n",
    "        out = wei@v\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embed,n_embed)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,x):\n",
    "        out =  torch.cat([h(x) for h in self.heads], dim = -1)\n",
    "        out = self.proj(out) # Projection si the linear transformation of the outcome of prev layer\n",
    "        return out\n",
    "\n",
    "class SinusoidalActivation(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return torch.sin(x)\n",
    "        # return x + torch.sin(x) ** 2\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embed):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embed,4* n_embed), \n",
    "            # nn.ReLU(),\n",
    "            nn.GELU(),\n",
    "            # SinusoidalActivation(),\n",
    "            nn.Linear(4* n_embed, n_embed),\n",
    "            nn.Dropout(dropout),\n",
    "            )\n",
    "        self\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embed, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embed //n_head\n",
    "        self.sa = MultiHeadAttention(n_head,head_size)\n",
    "        self.ffwd = FeedForward(n_embed)\n",
    "        self.ln1 = nn.LayerNorm(n_embed)\n",
    "        self.ln2 = nn.LayerNorm(n_embed)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = x + self.sa(self.ln1(x)) # add x for residual connections\n",
    "        x = x + self.ffwd(self.ln1(x))\n",
    "        return x\n",
    "\n",
    "# bigram language model\n",
    "class LanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
    "        self.position_embedding_table = nn.Embedding(block_size,n_embed)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embed,n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embed)\n",
    "        self.lm_head = nn.Linear(n_embed,vocab_size)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def lossy(self, output, target):\n",
    "        output = output.view(target.shape)  # Reshape output to match target's shape\n",
    "        return torch.mean((output != target).float())\n",
    "\n",
    "    def hamming_loss(self, predictions, targets):\n",
    "        # Ensure predictions are binary (0 or 1)\n",
    "        predictions = predictions > 0.5\n",
    "        # Calculate mismatches\n",
    "        mismatches = torch.ne(predictions.float(), targets.float()).float()\n",
    "        # Calculate the average number of mismatches\n",
    "        return torch.mean(mismatches)\n",
    "\n",
    "    def forward(self,idx,targets=None):\n",
    "        B,T = idx.shape\n",
    "        tok_emb = self.token_embedding_table(idx)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device = device))\n",
    "        x = tok_emb+pos_emb\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        # print(f\"logits are shape {logits.shape} are: {logits} for idx: {idx}\")\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(-1, vocab_size)  # Reshape logits to [batch_size * block_size, vocab_size]\n",
    "            targets = targets.view(-1)  # Flatten targets to [batch_size * block_size]\n",
    "            # loss = self.lossy(logits, targets)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "            # predictions = torch.sigmoid(logits) > 0.5\n",
    "            # loss = self.hamming_loss(predictions.view(-1), targets.view(-1))\n",
    "            # loss = F.binary_cross_entropy(torch.sigmoid(logits), targets.float())\n",
    "            # loss = F.mse_loss(logits, F.one_hot(targets, num_classes=vocab_size).float())\n",
    "            # print(f\"logits are shape {logits.shape} are: {loss} for idx: {idx}\")\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self,idx,max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, loss = self(idx_cond)\n",
    "            logits = logits[:,-1,:]\n",
    "            probs = F.softmax(logits,dim=-1)\n",
    "            idx_next = torch.multinomial(probs,num_samples=1)\n",
    "            idx=torch.cat((idx, idx_next), dim = 1)\n",
    "        return idx\n",
    "\n",
    "model = LanguageModel()\n",
    "m = model.to(device)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "# optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate, alpha=0.99, eps=1e-08, weight_decay=0.01, momentum=0.5, centered=False)\n",
    "optimizer = torch.optim.Adadelta(model.parameters(), lr=learning_rate, rho=0.9, eps=1e-06, weight_decay=0.01)\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "# scheduler = StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.2, patience=1, verbose=True)\n",
    "loss = None  # Initialize loss variable outside the loop\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for iter in range(max_iter // epochs):  # Distribute iterations across epochs\n",
    "        model.train()\n",
    "        xb, yb = get_batch(batch_size, grid_size, step_count, block_size)\n",
    "        logits, loss = model(xb, yb)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        max_norm = 0.9\n",
    "        # Clip gradients to avoid exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
    "        optimizer.step()\n",
    "\n",
    "        if iter % eval_interval == 0 and loss is not None:  # Validation logic\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                xv, yv = get_batch(val_batch_size, grid_size, step_count, block_size)\n",
    "                val_logits, val_loss = model(xv, yv)\n",
    "                print(f\"Epoch {epoch}, Iteration {iter}: Training Loss = {loss.item()}, Validation Loss = {val_loss.item()}\")\n",
    "            model.train()\n",
    "\n",
    "    scheduler.step(val_loss)  # Update the learning rate at the end of each epoch\n",
    "\n",
    "# Save:\n",
    "torch.save(model, 'cat_cgol_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input is: 0100000000100100110000000001001011111010110011111100001001110010\n",
      "Generated from CA is: 1101111010100000011111110011110110101010000010100001101001100101\n",
      "\n",
      "Generated from T is: 100000000100100110000000001001011111010110011111100001001110010111110010100111000011100000101010111101010111010010101110011101\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126\n",
      "64\n",
      "62\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model = torch.load('cat_cgol_model.pth')\n",
    "model.eval()\n",
    "input_length = 8\n",
    "generations = 1\n",
    "input_sequence = ''.join(np.random.choice(tokens, input_length*input_length))\n",
    "print(f\"Input is: {input_sequence}\")\n",
    "context = torch.tensor(enc(input_sequence), dtype=torch.long, device=device).unsqueeze(0)\n",
    "output = model.generate(context, max_new_tokens=len(input_sequence))\n",
    "generated_text_t = dec(output[0].tolist())\n",
    "game = GameOfLife(input_sequence, generations=generations)\n",
    "\n",
    "generated_text_ca = game.run_simulation()\n",
    "\n",
    "def visualize_grid_with_modifiers(grid):\n",
    "    \"\"\"Visualise the grid.\"\"\"\n",
    "    base_colors = {'0': 'red', '1': 'blue', 's': 'grey', 'e': 'black'}\n",
    "    colors = []\n",
    "    for row in grid:\n",
    "        row_colors = [base_colors[base] for base in row]\n",
    "        colors.extend(row_colors)\n",
    "\n",
    "    plt.figure(figsize=(20, 2))\n",
    "    plt.bar(range(len(colors)), np.ones(len(colors)), color=colors)\n",
    "    plt.axis('off')\n",
    "    plt.title('Visualization')\n",
    "    plt.show()\n",
    "\n",
    "print(f\"Generated from CA is: {generated_text_ca}\\n\")\n",
    "print(f\"Generated from T is: {generated_text_t}\\n\")\n",
    "visualize_grid_with_modifiers(input_sequence)\n",
    "visualize_grid_with_modifiers(generated_text_ca)\n",
    "visualize_grid_with_modifiers(generated_text_t)  # Use the fixed, split decoded output\n",
    "\n",
    "print(len(generated_text_t))\n",
    "print(len(input_sequence))\n",
    "print(len(generated_text_t)-(len(input_sequence)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
