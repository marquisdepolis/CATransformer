{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Iteration 0: Training Loss = 1.3473395109176636, Validation Loss = 0.9149006605148315\n",
      "Epoch 1, Iteration 0: Training Loss = 0.6706315875053406, Validation Loss = 0.6664126515388489\n",
      "Epoch 2, Iteration 0: Training Loss = 0.6700578331947327, Validation Loss = 0.6654688715934753\n",
      "Epoch 3, Iteration 0: Training Loss = 0.6727769374847412, Validation Loss = 0.6659235954284668\n",
      "Epoch 4, Iteration 0: Training Loss = 0.6764578819274902, Validation Loss = 0.6679898500442505\n",
      "Epoch 5, Iteration 0: Training Loss = 0.6651329398155212, Validation Loss = 0.6675012111663818\n",
      "Epoch 00006: reducing learning rate of group 0 to 5.0000e-01.\n",
      "Epoch 6, Iteration 0: Training Loss = 0.6646775007247925, Validation Loss = 0.667564868927002\n",
      "Epoch 7, Iteration 0: Training Loss = 0.6708869338035583, Validation Loss = 0.6663230061531067\n",
      "Epoch 8, Iteration 0: Training Loss = 0.6682197451591492, Validation Loss = 0.6684682965278625\n",
      "Epoch 00009: reducing learning rate of group 0 to 2.5000e-01.\n",
      "Epoch 9, Iteration 0: Training Loss = 0.6842027902603149, Validation Loss = 0.6695631742477417\n"
     ]
    }
   ],
   "source": [
    "# transformer plus cellular automata: conway game of life\n",
    "import torch\n",
    "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn \n",
    "from torch.nn import functional as F\n",
    "from gameoflife import GameOfLife\n",
    "\n",
    "batch_size = 8\n",
    "block_size = 256 # Has to be the square of grid_size to read the full grid\n",
    "max_iter = 5000\n",
    "epochs = 10\n",
    "eval_interval = 500\n",
    "learning_rate = 1e-4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 100\n",
    "n_embed = 128\n",
    "n_head = 8\n",
    "n_layer = 16\n",
    "dropout = 0.2\n",
    "text = []\n",
    "\n",
    "# create dictionaries and then define unique characters for encoding and decoding\n",
    "tokens = ['0', '1', 's', 'e']\n",
    "\n",
    "# Example usage with cellular automata\n",
    "grid_size = 16  # Grid size for the cellular automata\n",
    "step_count = 1  # Number of steps to evolve the cellular automata\n",
    "\n",
    "vocab_size=len(tokens)\n",
    "stoi = { ch:i for i, ch in enumerate(tokens)}\n",
    "itos = { i:ch for i, ch in enumerate(tokens)}\n",
    "enc = lambda s: [stoi['s']] + [stoi[c] for c in s] + [stoi['e']]\n",
    "dec = lambda l: ''.join([itos[i] for i in l[1:-1]])\n",
    "\n",
    "def generate_random_input_string(size):\n",
    "    \"\"\"Generate a random grid as a string for a given grid size.\"\"\"\n",
    "    return ''.join(np.random.choice(tokens[:2], size*size))\n",
    "\n",
    "def generate_game_of_life_sequence(batch_size, grid_size, step_count):\n",
    "    \"\"\"Generate a batch of Game of Life initial states and final states.\"\"\"\n",
    "    initial_states = [generate_random_input_string(grid_size) for _ in range(batch_size)]\n",
    "    final_states = []\n",
    "    for state in initial_states:\n",
    "        # Directly use state without 's' and 'e' tokens for simulation\n",
    "        game = GameOfLife(input_string=state, generations=step_count)\n",
    "        final_state = game.run_simulation()  # Assuming this returns the final state string\n",
    "        # Only add 's' and 'e' tokens for neural network processing, not for GameOfLife simulation\n",
    "        final_state_with_tokens = 's' + final_state + 'e'\n",
    "        final_states.append(final_state_with_tokens)\n",
    "    # Add 's' and 'e' tokens to initial states after simulation to maintain consistency\n",
    "    initial_states_with_tokens = ['s' + state + 'e' for state in initial_states]\n",
    "    return initial_states_with_tokens, final_states\n",
    "\n",
    "# Define an appropriate size for your validation batch\n",
    "val_batch_size = 20  \n",
    "\n",
    "# load data\n",
    "def get_batch(batch_size, grid_size, step_count, block_size):\n",
    "    initial_states, final_states = generate_game_of_life_sequence(batch_size, grid_size, step_count)\n",
    "    X = torch.tensor([enc(s)[:block_size] for s in initial_states], dtype=torch.long)\n",
    "    Y = torch.tensor([enc(s)[:block_size] for s in final_states], dtype=torch.long)\n",
    "    return X.to(device), Y.to(device)\n",
    "\n",
    "# single head attention\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embed,head_size,bias=False)\n",
    "        self.query = nn.Linear(n_embed,head_size,bias=False)\n",
    "        self.value = nn.Linear(n_embed,head_size,bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size,block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        wei = q @ k.transpose(-2,-1) *C**-0.5 # scaled attention\n",
    "        # wei = wei.masked_fill(self.tril[:T,:T]==0,float('-inf')) # decoder block\n",
    "        wei = F.softmax(wei,dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "        v = self.value(x)\n",
    "        out = wei@v\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embed,n_embed)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,x):\n",
    "        out =  torch.cat([h(x) for h in self.heads], dim = -1)\n",
    "        out = self.proj(out) # Projection si the linear transformation of the outcome of prev layer\n",
    "        return out\n",
    "\n",
    "class SinusoidalActivation(nn.Module):\n",
    "    def forward(self, x):\n",
    "        # return torch.sin(x)\n",
    "        return x + torch.sin(x) ** 2\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embed):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embed,4* n_embed), \n",
    "            # nn.ReLU(),\n",
    "            nn.GELU(),\n",
    "            # SinusoidalActivation(),\n",
    "            nn.Linear(4* n_embed, n_embed),\n",
    "            nn.Dropout(dropout),\n",
    "            )\n",
    "        self\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embed, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embed //n_head\n",
    "        self.sa = MultiHeadAttention(n_head,head_size)\n",
    "        self.ffwd = FeedForward(n_embed)\n",
    "        self.ln1 = nn.LayerNorm(n_embed)\n",
    "        self.ln2 = nn.LayerNorm(n_embed)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = x + self.sa(self.ln1(x)) # add x for residual connections\n",
    "        x = x + self.ffwd(self.ln1(x))\n",
    "        return x\n",
    "\n",
    "# bigram language model\n",
    "class LanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
    "        self.position_embedding_table = nn.Embedding(block_size,n_embed)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embed,n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embed)\n",
    "        self.lm_head = nn.Linear(n_embed,vocab_size)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def lossy(self, output, target):\n",
    "        output = output.view(target.shape)  # Reshape output to match target's shape\n",
    "        return torch.mean((output != target).float())\n",
    "\n",
    "    def forward(self,idx,targets=None):\n",
    "        B,T = idx.shape\n",
    "        tok_emb = self.token_embedding_table(idx)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device = device))\n",
    "        x = tok_emb+pos_emb\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        # print(f\"logits are shape {logits.shape} are: {logits} for idx: {idx}\")\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(-1, vocab_size)  # Reshape logits to [batch_size * block_size, vocab_size]\n",
    "            targets = targets.view(-1)  # Flatten targets to [batch_size * block_size]\n",
    "            # loss = self.lossy(logits, targets)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "            # loss = F.binary_cross_entropy(logits, targets)\n",
    "            # loss = F.mse_loss(logits, F.one_hot(targets, num_classes=vocab_size).float())\n",
    "            # print(f\"logits are shape {logits.shape} are: {loss} for idx: {idx}\")\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self,idx,max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, loss = self(idx_cond)\n",
    "            logits = logits[:,-1,:]\n",
    "            probs = F.softmax(logits,dim=-1)\n",
    "            idx_next = torch.multinomial(probs,num_samples=1)\n",
    "            idx=torch.cat((idx, idx_next), dim = 1)\n",
    "        return idx\n",
    "\n",
    "model = LanguageModel()\n",
    "m = model.to(device)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "# optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0, centered=False)\n",
    "optimizer = torch.optim.Adadelta(model.parameters(), lr=1.0, rho=0.9, eps=1e-06, weight_decay=0.01)\n",
    "# scheduler = StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=2, verbose=True)\n",
    "loss = None  # Initialize loss variable outside the loop\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for iter in range(max_iter // epochs):  # Distribute iterations across epochs\n",
    "        model.train()\n",
    "        xb, yb = get_batch(batch_size, grid_size, step_count, block_size)\n",
    "        logits, loss = model(xb, yb)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        max_norm = 1.0\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
    "        optimizer.step()\n",
    "\n",
    "        if iter % eval_interval == 0 and loss is not None:  # Validation logic\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                xv, yv = get_batch(val_batch_size, grid_size, step_count, block_size)\n",
    "                val_logits, val_loss = model(xv, yv)\n",
    "                print(f\"Epoch {epoch}, Iteration {iter}: Training Loss = {loss.item()}, Validation Loss = {val_loss.item()}\")\n",
    "            model.train()\n",
    "\n",
    "    scheduler.step(val_loss)  # Update the learning rate at the end of each epoch\n",
    "\n",
    "# Save:\n",
    "torch.save(model, 'cat_cgol_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input is: 1111011010110001101000100101001010111001001000010011100100110111\n",
      "\n",
      "Generated from CA is: 1000010010011100101111100101111010111111101001111110110100000101\n",
      "\n",
      "\n",
      "Generated from T is: 000011110001101100001011111111100010111010110000111100011000101\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model = torch.load('cat_cgol_model.pth')\n",
    "model.eval()\n",
    "input_length = 8\n",
    "generations = 1\n",
    "input_sequence = ''.join(np.random.choice(tokens[:2], input_length*input_length))\n",
    "print(f\"Input is: {input_sequence}\")\n",
    "context = torch.tensor(enc(input_sequence), dtype=torch.long, device=device).unsqueeze(0)\n",
    "output = model.generate(context, max_new_tokens=len(input_sequence))\n",
    "generated_text_t = dec(output[0].tolist())\n",
    "game = GameOfLife(input_sequence, generations=generations)\n",
    "\n",
    "if generated_text_t.startswith('s'):\n",
    "    generated_text_t = generated_text_t[1:]\n",
    "\n",
    "generated_text_ca = game.run_simulation()\n",
    "_, model_generated_sequence = generated_text_t.split('e', 1)\n",
    "\n",
    "def visualize_grid_with_modifiers(grid):\n",
    "    \"\"\"Visualise the grid.\"\"\"\n",
    "    base_colors = {'0': 'red', '1': 'blue', ' ': 'grey'}\n",
    "    colors = []\n",
    "    for row in grid:\n",
    "        row_colors = [base_colors[base] for base in row]\n",
    "        colors.extend(row_colors)\n",
    "\n",
    "    plt.figure(figsize=(20, 2))\n",
    "    plt.bar(range(len(colors)), np.ones(len(colors)), color=colors)\n",
    "    plt.axis('off')\n",
    "    plt.title('Visualization with Modifiers')\n",
    "    plt.show()\n",
    "\n",
    "print(f\"\\nGenerated from CA is: {generated_text_ca}\\n\")\n",
    "print(f\"\\nGenerated from T is: {model_generated_sequence}\\n\")\n",
    "visualize_grid_with_modifiers(generated_text_ca)\n",
    "visualize_grid_with_modifiers(model_generated_sequence)  # Use the fixed, split decoded output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
