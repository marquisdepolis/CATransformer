{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Iteration 0: Training Loss = 1.3830347061157227, Validation Loss = 0.843181312084198\n",
      "Epoch 1, Iteration 0: Training Loss = 0.6584060192108154, Validation Loss = 0.6537281274795532\n",
      "Epoch 2, Iteration 0: Training Loss = 0.6409439444541931, Validation Loss = 0.6384798288345337\n",
      "Epoch 3, Iteration 0: Training Loss = 0.6605014801025391, Validation Loss = 0.6448599696159363\n",
      "Epoch 4, Iteration 0: Training Loss = 0.6492700576782227, Validation Loss = 0.6420793533325195\n",
      "Epoch 5, Iteration 0: Training Loss = 0.651297926902771, Validation Loss = 0.6410773396492004\n",
      "Epoch 00006: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 6, Iteration 0: Training Loss = 0.6442320942878723, Validation Loss = 0.6358736753463745\n",
      "Epoch 7, Iteration 0: Training Loss = 0.6496292948722839, Validation Loss = 0.6425917148590088\n",
      "Epoch 8, Iteration 0: Training Loss = 0.6413035988807678, Validation Loss = 0.6472872495651245\n",
      "Epoch 9, Iteration 0: Training Loss = 0.6464611887931824, Validation Loss = 0.6390292048454285\n",
      "Epoch 00010: reducing learning rate of group 0 to 2.5000e-04.\n"
     ]
    }
   ],
   "source": [
    "# transformer plus cellular automata: conway game of life\n",
    "import torch\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn \n",
    "from torch.nn import functional as F\n",
    "from gameoflife import GameOfLife\n",
    "\n",
    "batch_size = 8\n",
    "block_size = 256 # Has to be the square of grid_size to read the full grid\n",
    "max_iter = 2000\n",
    "epochs = 10\n",
    "eval_interval = 500\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 100\n",
    "n_embed = 128\n",
    "n_head = 8\n",
    "n_layer = 16\n",
    "dropout = 0.2\n",
    "text = []\n",
    "\n",
    "# create dictionaries and then define unique characters for encoding and decoding\n",
    "tokens = ['0', '1', 's', 'e']\n",
    "\n",
    "# Example usage with cellular automata\n",
    "grid_size = 16  # Grid size for the cellular automata\n",
    "step_count = 2  # Number of steps to evolve the cellular automata\n",
    "\n",
    "vocab_size=len(tokens)\n",
    "stoi = { ch:i for i, ch in enumerate(tokens)}\n",
    "itos = { i:ch for i, ch in enumerate(tokens)}\n",
    "enc = lambda s: [stoi['s']] + [stoi[c] for c in s] + [stoi['e']]\n",
    "dec = lambda l: ''.join([itos[i] for i in l[1:-1]])\n",
    "\n",
    "def generate_random_input_string(size):\n",
    "    \"\"\"Generate a random grid as a string for a given grid size.\"\"\"\n",
    "    return ''.join(np.random.choice(tokens[:2], size*size))\n",
    "\n",
    "def generate_game_of_life_sequence(batch_size, grid_size, step_count):\n",
    "    \"\"\"Generate a batch of Game of Life initial states and final states.\"\"\"\n",
    "    initial_states = [generate_random_input_string(grid_size) for _ in range(batch_size)]\n",
    "    final_states = []\n",
    "    for state in initial_states:\n",
    "        # Directly use state without 's' and 'e' tokens for simulation\n",
    "        game = GameOfLife(input_string=state, generations=step_count)\n",
    "        final_state = game.run_simulation()  # Assuming this returns the final state string\n",
    "        # Only add 's' and 'e' tokens for neural network processing, not for GameOfLife simulation\n",
    "        final_state_with_tokens = 's' + final_state + 'e'\n",
    "        final_states.append(final_state_with_tokens)\n",
    "    # Add 's' and 'e' tokens to initial states after simulation to maintain consistency\n",
    "    initial_states_with_tokens = ['s' + state + 'e' for state in initial_states]\n",
    "    return initial_states_with_tokens, final_states\n",
    "\n",
    "# Define an appropriate size for your validation batch\n",
    "val_batch_size = 20  \n",
    "\n",
    "# load data\n",
    "def get_batch(batch_size, grid_size, step_count, block_size):\n",
    "    initial_states, final_states = generate_game_of_life_sequence(batch_size, grid_size, step_count)\n",
    "    X = torch.tensor([enc(s)[:block_size] for s in initial_states], dtype=torch.long)\n",
    "    Y = torch.tensor([enc(s)[:block_size] for s in final_states], dtype=torch.long)\n",
    "    return X.to(device), Y.to(device)\n",
    "\n",
    "# single head attention\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embed,head_size,bias=False)\n",
    "        self.query = nn.Linear(n_embed,head_size,bias=False)\n",
    "        self.value = nn.Linear(n_embed,head_size,bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size,block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        wei = q @ k.transpose(-2,-1) *C**-0.5 # scaled attention\n",
    "        wei = wei.masked_fill(self.tril[:T,:T]==0,float('-inf')) # decoder block\n",
    "        wei = F.softmax(wei,dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "        v = self.value(x)\n",
    "        out = wei@v\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embed,n_embed)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,x):\n",
    "        out =  torch.cat([h(x) for h in self.heads], dim = -1)\n",
    "        out = self.proj(out) # Projection si the linear transformation of the outcome of prev layer\n",
    "        return out\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embed):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embed,4* n_embed), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4* n_embed, n_embed),\n",
    "            nn.Dropout(dropout),\n",
    "            )\n",
    "        self\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embed, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embed //n_head\n",
    "        self.sa = MultiHeadAttention(n_head,head_size)\n",
    "        self.ffwd = FeedForward(n_embed)\n",
    "        self.ln1 = nn.LayerNorm(n_embed)\n",
    "        self.ln2 = nn.LayerNorm(n_embed)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = x + self.sa(self.ln1(x)) # add x for residual connections\n",
    "        x = x + self.ffwd(self.ln1(x))\n",
    "        return x\n",
    "\n",
    "# bigram language model\n",
    "class LanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
    "        self.position_embedding_table = nn.Embedding(block_size,n_embed)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embed,n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embed)\n",
    "        self.lm_head = nn.Linear(n_embed,vocab_size)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self,idx,targets=None):\n",
    "        B,T = idx.shape\n",
    "        tok_emb = self.token_embedding_table(idx)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device = device))\n",
    "        x = tok_emb+pos_emb\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        # print(f\"logits are shape {logits.shape} are: {logits} for idx: {idx}\")\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(-1, vocab_size)  # Reshape logits to [batch_size * block_size, vocab_size]\n",
    "            targets = targets.view(-1)  # Flatten targets to [batch_size * block_size]\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "            # loss = F.mse_loss(logits, F.one_hot(targets, num_classes=vocab_size).float())\n",
    "\n",
    "            # print(f\"logits are shape {logits.shape} are: {loss} for idx: {idx}\")\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self,idx,max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, loss = self(idx_cond)\n",
    "            logits = logits[:,-1,:]\n",
    "            probs = F.softmax(logits,dim=-1)\n",
    "            idx_next = torch.multinomial(probs,num_samples=1)\n",
    "            idx=torch.cat((idx, idx_next), dim = 1)\n",
    "        return idx\n",
    "    \n",
    "model = LanguageModel()\n",
    "m = model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "# scheduler = StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=2, verbose=True)\n",
    "loss = None  # Initialize loss variable outside the loop\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for iter in range(max_iter // epochs):  # Distribute iterations across epochs\n",
    "        model.train()\n",
    "        xb, yb = get_batch(batch_size, grid_size, step_count, block_size)\n",
    "        logits, loss = model(xb, yb)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if iter % eval_interval == 0 and loss is not None:  # Validation logic\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                xv, yv = get_batch(val_batch_size, grid_size, step_count, block_size)\n",
    "                val_logits, val_loss = model(xv, yv)\n",
    "                print(f\"Epoch {epoch}, Iteration {iter}: Training Loss = {loss.item()}, Validation Loss = {val_loss.item()}\")\n",
    "            model.train()\n",
    "\n",
    "    scheduler.step(val_loss)  # Update the learning rate at the end of each epoch\n",
    "\n",
    "# Save:\n",
    "torch.save(model, 'cat_cgol_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input is: 0011111001001100001101010010101100100111100111010010011100101000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated from CA is: 0011011000011100000110000000111100001111000101100000101100000100\n",
      "\n",
      "\n",
      "Generated from T is: 111100111111110010000011000100000001101001011101001111110111110\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model = torch.load('cat_cgol_model.pth')\n",
    "model.eval()\n",
    "input_length = 8\n",
    "generations = 2\n",
    "input_sequence = ''.join(np.random.choice(tokens[:2], input_length*input_length))\n",
    "print(f\"Input is: {input_sequence}\")\n",
    "context = torch.tensor(enc(input_sequence), dtype=torch.long, device=device).unsqueeze(0)\n",
    "output = model.generate(context, max_new_tokens=len(input_sequence))\n",
    "generated_text_t = dec(output[0].tolist())\n",
    "game = GameOfLife(input_sequence, generations=generations)\n",
    "\n",
    "if generated_text_t.startswith('s'):\n",
    "    generated_text_t = generated_text_t[1:]\n",
    "\n",
    "generated_text_ca = game.run_simulation()\n",
    "_, model_generated_sequence = generated_text_t.split('e', 1)\n",
    "\n",
    "def visualize_grid_with_modifiers(grid):\n",
    "    \"\"\"Visualise the grid.\"\"\"\n",
    "    base_colors = {'0': 'red', '1': 'blue', ' ': 'grey'}\n",
    "    colors = []\n",
    "    for row in grid:\n",
    "        row_colors = [base_colors[base] for base in row]\n",
    "        colors.extend(row_colors)\n",
    "\n",
    "    plt.figure(figsize=(20, 2))\n",
    "    plt.bar(range(len(colors)), np.ones(len(colors)), color=colors)\n",
    "    plt.axis('off')\n",
    "    plt.title('Visualization with Modifiers')\n",
    "    plt.show()\n",
    "\n",
    "print(f\"\\nGenerated from CA is: {generated_text_ca}\\n\")\n",
    "print(f\"\\nGenerated from T is: {model_generated_sequence}\\n\")\n",
    "visualize_grid_with_modifiers(generated_text_ca)\n",
    "visualize_grid_with_modifiers(model_generated_sequence)  # Use the fixed, split decoded output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
